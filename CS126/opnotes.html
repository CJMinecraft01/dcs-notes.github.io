<!DOCTYPE html>
<html lang=" en-US">

<head>

    
    <meta charset="UTF-8"><!-- Include tocNAV javascript -->
    <script type="text/javascript" src="/dcs-notes.github.io/assets/js/tocNav.js"></script>

    <!-- seo used to be here -->
    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link rel="preload" href="https://fonts.googleapis.com/css?family=Open+Sans:400,700&display=swap" as="style"
        type="text/css" crossorigin>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <link rel="stylesheet" href="/dcs-notes.github.io/assets/css/style.css">
</head>

<body>
    <div id="mainGrid" class="container">
        <header style="padding:10px;" class="page-header notes-header" role="banner">
            <h1 class="project-name">126 One Page Notes</h1>
        </header>
        <div title="Table of Contents" class="buttonCol" onclick="toggleNav()">
            <div class="navArrow">
                <i></i>
            </div>
        </div>
        <div class="navBox">
            <div id="sidenav" class="sideNav closedNav">
                <h2 style="margin-left: 10px;">Table of Contents</h2><ul><li><a href="#arrays-and-lists">Arrays and Lists</a><ul><li><a href="#arrays-adt">Arrays (ADT)</a><ul><li><a href="#implementation">Implementation</a></li></ul></li><li><a href="#lists-adt">Lists (ADT)</a><ul><li><a href="#array-based-implementation">Array based implementation</a></li></ul></li><li><a href="#positional-lists-adt">Positional lists (ADT)</a></li><li><a href="#linked-lists-adt">Linked lists (ADT)</a><ul><li><a href="#singly-linked-lists">Singly linked lists</a></li><li><a href="#doubly-linked-lists">Doubly Linked Lists</a></li></ul></li></ul></li><li><a href="#analysis-of-algorithms">Analysis of algorithms</a><ul><li><a href="#running-time">Running time</a></li><li><a href="#experimental-trials">Experimental trials</a></li><li><a href="#theoretical-analysis">Theoretical analysis</a></li><li><a href="#common-functions-of-running-time">Common functions of running time</a></li><li><a href="#random-access-machine-ram-model">Random Access Machine (RAM) model</a></li><li><a href="#big-o-notation">Big-O Notation</a><ul><li><a href="#big-o-of-a-function">Big-O of a Function</a></li></ul></li><li><a href="#asymptotic-algorithm-analysis">Asymptotic Algorithm Analysis</a></li><li><a href="#relatives-of-big-o">Relatives of Big-O</a></li></ul></li><li><a href="#recursive-algorithms">Recursive algorithms</a><ul><li><a href="#definition">Definition</a></li><li><a href="#structure">Structure</a></li><li><a href="#examples">Examples</a></li><li><a href="#types-of-recursion">Types of recursion</a></li></ul></li><li><a href="#stacks-and-queues">Stacks and Queues</a><ul><li><a href="#stacks-adt">Stacks (ADT)</a><ul><li><a href="#array-based-implementation">Array Based Implementation</a></li></ul></li><li><a href="#queues-adt">Queues (ADT)</a><ul><li><a href="#array-based-implementation-1">Array Based Implementation</a></li></ul></li></ul></li><li><a href="#maps-hash-tables-and-sets">Maps, Hash tables and Sets</a><ul><li><a href="#maps-adt">Maps (ADT)</a></li><li><a href="#hash-tables">Hash tables</a><ul><li><a href="#hash-functions">Hash functions</a></li><li><a href="#memory-address">Memory address</a></li><li><a href="#integer-cast">Integer cast</a></li><li><a href="#component-sum">Component sum</a></li><li><a href="#polynomial-accumulation">Polynomial accumulation</a></li><li><a href="#java-hash-implementations">Java hash implementations</a></li></ul></li><li><a href="#collisions">Collisions</a><ul><li><a href="#separate-chaining">Separate Chaining</a></li><li><a href="#linear-probing">Linear Probing</a></li><li><a href="#double-hashing">Double Hashing</a></li></ul></li><li><a href="#resizing-a-hash-table">Resizing a hash table</a></li><li><a href="#performance-of-hashing">Performance of Hashing</a></li><li><a href="#sets-adt">Sets (ADT)</a></li><li><a href="#implementations">Implementations</a><ul><li><a href="#list-based">List based</a><ul><li><a href="#generic-merging-algorithm">Generic Merging Algorithm</a></li></ul></li><li><a href="#hash-set-based">Hash-set based</a></li></ul></li></ul></li><li><a href="#trees">Trees</a><ul><li><a href="#trees-adt">Trees (ADT)</a></li><li><a href="#tree-traversals">Tree Traversals</a><ul><li><a href="#in-order-traversal">In-Order Traversal</a></li><li><a href="#pre-order-traversal">Pre-order traversal</a></li><li><a href="#post-order-traversal">Post-order traversal</a></li></ul></li><li><a href="#binary-trees-adt">Binary trees (ADT)</a><ul><li><a href="#properties">Properties</a></li><li><a href="#implementations">Implementations</a></li><li><a href="#linked-structure">Linked structure</a></li><li><a href="#array-based">Array based</a></li></ul></li></ul></li><li><a href="#priority-queues">Priority queues</a><ul><li><a href="#priority-queues-adt">Priority queues (ADT)</a></li><li><a href="#implementations">Implementations</a><ul><li><a href="#unsorted-list-based">Unsorted list based</a></li><li><a href="#sorted-list-based">Sorted list based</a></li><li><a href="#heap-based">Heap based</a></li></ul></li><li><a href="#comparators">Comparators</a></li><li><a href="#sorting-with-list-based-priority-queues">Sorting with list based priority queues</a></li></ul></li><li><a href="#heaps">Heaps</a><ul><li><a href="#heaps-adt">Heaps (ADT)</a></li><li><a href="#heap-properties">Heap properties</a><ul><li><a href="#height-of-a-heap">Height of a Heap</a></li></ul></li><li><a href="#heap-methods">Heap methods</a><ul><li><a href="#inserting-into-a-heap">Inserting into a heap</a></li><li><a href="#removal-from-a-heap">Removal from a heap</a></li></ul></li><li><a href="#use-in-sorting">Use in sorting</a></li><li><a href="#concrete-implementations">Concrete implementations</a></li></ul></li><li><a href="#skip-lists">Skip Lists</a><ul><li><a href="#motivations-for-skip-lists">Motivations for skip lists</a></li><li><a href="#skip-lists-adt">Skip Lists (ADT)</a></li><li><a href="#searching">Searching</a></li><li><a href="#inserting">Inserting</a></li><li><a href="#deleting">Deleting</a></li><li><a href="#implementation">Implementation</a></li><li><a href="#performance">Performance</a><ul><li><a href="#space-usage">Space usage</a></li><li><a href="#height">Height</a></li><li><a href="#search-time">Search time</a></li><li><a href="#update-time">Update time</a></li><li><a href="#coin-toss-expectation-explanation">Coin Toss Expectation Explanation</a></li></ul></li></ul></li><li><a href="#binary-search-and-self-balancing-trees">Binary search and self-balancing trees</a></li><li><a href="#binary-search-trees">Binary search trees</a><ul><li><a href="#properties">Properties</a></li><li><a href="#operations">Operations</a><ul><li><a href="#searching">Searching</a></li><li><a href="#insertion">Insertion</a></li><li><a href="#deletion">Deletion</a></li></ul></li><li><a href="#algorithm">Algorithm</a></li></ul></li><li><a href="#avl-trees">AVL trees</a><ul><li><a href="#properties-of-a-balanced-tree">Properties of a balanced tree</a></li><li><a href="#operations-1">Operations</a><ul><li><a href="#tri-node-restructuring">Tri-node restructuring</a></li><li><a href="#re-balancing-vs-restructuring">Re-balancing VS restructuring</a></li><li><a href="#insertion-1">Insertion</a></li><li><a href="#deletion-1">Deletion</a></li></ul></li><li><a href="#performance">Performance</a></li></ul></li><li><a href="#graphs">Graphs</a></li><li><a href="#graphs">Graphs</a><ul><li><a href="#graphs-as-a-mathematical-concept">Graphs as a mathematical concept</a><ul><li><a href="#terminology">Terminology</a></li><li><a href="#graph-properties">Graph properties</a></li></ul></li><li><a href="#graphs-as-an-adt">Graphs as an ADT</a></li><li><a href="#more-terminology">More terminology</a><ul><li><a href="#subgraphs">Subgraphs</a></li></ul></li></ul></li><li><a href="#depth-first-search">Depth-first search</a></li><li><a href="#breadth-first-search">Breadth-first search</a></li><li><a href="#directed-graphs">Directed graphs</a><ul><li><a href="#transitive-closure">Transitive closure</a></li><li><a href="#topological-ordering">Topological ordering</a></li></ul></li><li><a href="#general-algorithms">General algorithms</a></li><li><a href="#searching-data-structures">Searching data structures</a><ul><li><a href="#linear-search">Linear search</a></li><li><a href="#binary-search">Binary search</a><ul><li><a href="#iterative-algorithm">Iterative algorithm</a></li><li><a href="#recursive-algorithm">Recursive algorithm</a></li></ul></li></ul></li><li><a href="#sorting-data-structures">Sorting data structures</a><ul><li><a href="#insertion-sort">Insertion sort</a></li><li><a href="#selection-sort">Selection sort</a></li><li><a href="#heap-sort">Heap sort</a></li><li><a href="#merge-sort">Merge sort</a></li></ul></li><li><a href="#reversing-data-structures">Reversing data structures</a><ul><li><a href="#reversing-a-stack">Reversing a stack</a></li><li><a href="#reversing-a-linked-list">Reversing a linked list</a></li></ul></li><li><a href="#set-operations">Set operations</a><ul><li><a href="#generic-merging">Generic merging</a></li></ul></li><li><a href="#graph-algorithms">Graph algorithms</a><ul><li><a href="#depth-first-search">Depth-first search</a><ul><li><a href="#dfs-for-an-entire-graph">DFS for an entire graph:</a></li><li><a href="#path-finding-with-dfs">Path Finding with DFS</a></li><li><a href="#cycle-finding-with-dfs">Cycle Finding with DFS</a></li><li><a href="#topological-ordering-using-dfs">Topological ordering using DFS</a></li></ul></li><li><a href="#breadth-first-search">Breadth-first search</a></li><li><a href="#directed-graphs">Directed graphs</a></li></ul></li><li><a href="#miscellaneous">Miscellaneous</a><ul><li><a href="#computing-spans">Computing spans</a></li><li><a href="#fibonacci">Fibonacci</a><ul><li><a href="#exponential-time">Exponential time</a></li><li><a href="#linear-time">Linear time</a></li></ul></li></ul></li></ul>
</div>
        </div>
        
        <div class="contents">
            <main id="content" class="main-content" role="main">
                <div class="partNav"><a href="./">🏡CS126</a></div>
                <!-- Main Content of markdown or sub-layouts-->
                <!-- Layout for One Page Notes -->
<!-- 
    Works for all modules as long as they 
    - Have part: true
    - Are defined as a collection in _config.yml
    - The folder name is "_[Module Code]"
--><h1 id="arrays-and-lists">Arrays and Lists</h1>
       
        <h2 id="arrays-adt">Arrays (ADT)</h2>

<blockquote>
  <p>Arrays are <strong>indexable</strong>, <strong>fixed length</strong>, sequence of variables of a <strong>single type</strong> (homogenous).</p>

  <ul>
    <li>They are homogenous as it is otherwise much harder to calculate the memory address of the data to look up given an index.</li>
  </ul>
</blockquote>

<p>This table is an overview of the time complexity of certain operations for an array.</p>

<table>
  <thead>
    <tr>
      <th>Methods/Operations</th>
      <th>Time</th>
      <th>Reason</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">get(i)</code>, <code class="language-plaintext highlighter-rouge">set(i,e)</code></td>
      <td>O(1)</td>
      <td>Indexable</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">size()</code></td>
      <td>O(1)</td>
      <td>Arrays are of fixed size when created, they know their size.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">isEmpty()</code></td>
      <td>O(n)</td>
      <td>Has to check every index</td>
    </tr>
    <tr>
      <td>Insertion, Deletion</td>
      <td>O(n)</td>
      <td>Fixed length – have to shift proceeding elements up or down to accommodate inserted/deleted element</td>
    </tr>
    <tr>
      <td>Resizing the array</td>
      <td>O(n)</td>
      <td>Fixed length – have to create a larger array, then copy contents over.</td>
    </tr>
  </tbody>
</table>

<p>The operations in <code class="language-plaintext highlighter-rouge">code</code> blocks are the <strong>fundamental operations</strong> of arrays.</p>

<h3 id="implementation">Implementation</h3>

<p>Arrays can be <strong>concretely</strong> implemented by <strong>allocating</strong> a <strong>contiguous section</strong> of memory, with cells being indexable by memory location, as the data at an index can be found at</p>

<p>[S + D \cdot I]</p>

<p>where \(S\) is the pointer to the start of the array, \(D\) is the size of the data type, and \(I\) is the index.</p>

<h2 id="lists-adt">Lists (ADT)</h2>
<blockquote>
  <p><strong>Lists</strong> are a finite sequence of ordered values, which may contain duplicates (more abstract than an array). A list is called homogenous if every element it contains is of the same type.</p>
</blockquote>

<h3 id="array-based-implementation">Array based implementation</h3>

<p><em>Concrete implementation of lists</em></p>

<blockquote>
  <p>Arrays provide all the required properties, except being able to change size. To “grow” an array, we make a new array of a larger size, and copy all the data across to it.</p>
</blockquote>

<p>To do this, we need to decide how large the new array should be. There are two strategies which are commonly used to do this:</p>

<ul>
  <li><strong>Incremental strategy</strong> – when the capacity is exceeded, grow it by a constant number of elements <strong><em>c</em></strong>
    <ul>
      <li>Amortized (average) time of each push is Ω(n<sup>2</sup>)</li>
      <li>Space grows linearly, so quite space efficient</li>
    </ul>
  </li>
  <li><strong>Doubling strategy</strong> – when the capacity is exceeded, double it
    <ul>
      <li>Amortized (average) time of each push is Ω(n)</li>
      <li>Space grows exponentially, so less space efficient</li>
    </ul>
  </li>
</ul>

<p>Array based implementations have the fundamental operations</p>

<ul>
  <li>size()</li>
  <li>isEmpty()</li>
  <li>get(i)</li>
  <li>set(i,e)</li>
  <li>add(i,e)</li>
  <li>remove(i)</li>
</ul>

<h2 id="positional-lists-adt">Positional lists (ADT)</h2>

<blockquote>
  <p><strong>Positional lists</strong> are a “general abstraction of a sequence of elements with the ability to identify the location of an element, without indices”</p>

  <p><em>“Data Structures and Algorithms in Java”, Goodrich, Tamassia, Goldwasser</em></p>
</blockquote>

<p>A “position” is a marker within the list, which is unaffected by changes elsewhere. For example, insertion or deletion of other elements doesn’t change it, the only time it changes is when it itself is deleted.</p>

<p>Fundamental operations</p>
<ul>
  <li>addFirst(e)</li>
  <li>addLast(e)</li>
  <li>addBefore(p,e)</li>
  <li>addAfter(p,e)</li>
  <li>set(p,e)</li>
  <li>remove(p)</li>
</ul>

<p>It is generally implemented as a <a href="#doubly-linked-lists">doubly linked list</a>.</p>

<h2 id="linked-lists-adt">Linked lists (ADT)</h2>

<blockquote>
  <p><strong>Linked lists</strong> are a collection of elements that can be accessed in a sequential way, meaning they are not indexable. <a href="https://lucasmagnum.medium.com/sidenotes-linked-list-abstract-data-type-and-data-structure-fd2f8276ab53">Additional resource.</a></p>
</blockquote>

<p>This means they can more easily implement non-homogenous lists, as opposed to using arrays, as cells can be of different “sizes”, so different data types requiring different amounts of data can be stored.</p>

<h3 id="singly-linked-lists">Singly linked lists</h3>

<p><em>Concrete implementation of linked lists</em></p>

<blockquote>
  <p><strong>Singly linked lists</strong> are a sequence of nodes, each of which stores both a value and a pointer to the next node in the sequence. There is a pointer to the first node in the sequence, and the final node in the sequence is a null pointer ∅</p>
</blockquote>

<table>
  <thead>
    <tr>
      <th>Method/Operation</th>
      <th>Time</th>
      <th>Reason | Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">set(p,e)</code>, <code class="language-plaintext highlighter-rouge">addAfter(p,e)</code>, get,</td>
      <td>O(n)</td>
      <td>Need to go through the list from head until index.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">addFirst(e)</code></td>
      <td>O(1)</td>
      <td>Quick to add items to head because we have a pointer reference</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">remove(p)</code> (Deletion), Insertion</td>
      <td>O(n)</td>
      <td>Similar to getting and <code class="language-plaintext highlighter-rouge">set</code>, but pointers are changed instead of values, either to bypass or include a new node in the sequence.</td>
    </tr>
  </tbody>
</table>

<p>Singly linked lists have the fundamental operations</p>
<ul>
  <li>
    <p>addFirst(e)</p>
  </li>
  <li>
    <p>addAfter(p,e)</p>
  </li>
  <li>
    <p>set(p,e)</p>
  </li>
  <li>
    <p>remove(p)</p>
  </li>
</ul>

<h3 id="doubly-linked-lists">Doubly Linked Lists</h3>

<p><em>Concrete implementation of positional lists and linked lists</em></p>

<blockquote>
  <p><strong>Doubly linked lists</strong> are a sequence of nodes, each of which stores both a value and a pointer to both the <strong>next</strong> and the <strong>previous</strong> node in the sequence. At <strong>each end</strong> there are <strong>special header</strong> and <strong>trailer nodes</strong>, which are <strong>just references</strong> to the first and last nodes in the sequence</p>
</blockquote>

<p>Similarly to singly linked lists, getting, setting, insertion, deletion all O(n) – need to iterate from start to end of the list to get to the position of the item.</p>

<p>Head and tail operations are O(1) – head and tail references (pointers) and the list can be traversed <strong>both</strong> forwards and backwards.</p>

<p>Fundamental operations (same as positional list as it is concrete implementation of it)</p>

<ul>
  <li>addFirst(e)</li>
  <li>addLast(e)</li>
  <li>addBefore(p,e)</li>
  <li>addAfter(p,e)</li>
  <li>set(p,e)</li>
  <li>remove(p)</li>
</ul>
<h1 id="analysis-of-algorithms">Analysis of algorithms</h1>
       
        <h2 id="running-time">Running time</h2>

<p>To assess how good an algorithm is, we often use the metric of running time compared with the size of the input to the algorithm.</p>

<ul>
  <li>Worst case – which we focus on here, since it is both easy to analyse and useful</li>
  <li>Average Case – often more difficult to assess</li>
  <li>Best Case – often not sufficiently representative of the algorithm</li>
</ul>

<h2 id="experimental-trials">Experimental trials</h2>

<blockquote>
  <p>One of the ways to assess the running time is to write a program implementing the algorithm, then running for inputs of different sizes. Then fit curves to a plot of the results to try to classify the algorithm.</p>
</blockquote>

<p>This has a few drawbacks though</p>

<ul>
  <li>Need to implement the algorithm – might be difficult.</li>
  <li>Many ways to implement – reason for analysis is to decide which one to implement</li>
  <li>Not all inputs can be covered – not representative</li>
  <li>Dependent on machine hardware and software environments – difficult to equate between different tests, same specs and same environment needed.</li>
</ul>

<h2 id="theoretical-analysis">Theoretical analysis</h2>

<blockquote>
  <p>Theoretical analysis is given a high-level description of the algorithm (not a full implementation), expressing the running time as a function of the input size \(n\).</p>

  <p>Pseudocode is used for this high-level description, which lies between English prose and program code. It has <strong>no formal syntax</strong>, and allows omission of some aspects of the implementation to make analysis easier.</p>
</blockquote>

<p>This has the benefits of:</p>

<ul>
  <li>Allowing all possible inputs to be covered</li>
  <li>Being independent of machine hardware and software environments, so easier to equate between different tests</li>
</ul>

<h2 id="common-functions-of-running-time">Common functions of running time</h2>

<p><img src="https://miro.medium.com/max/2928/1*5ZLci3SuR0zM_QlZOADv8Q.jpeg" alt="Complexity chart" /></p>

<p><a href="https://towardsdatascience.com/understanding-time-complexity-with-python-examples-2bda6e8158a7">Image source</a></p>

<h2 id="random-access-machine-ram-model">Random Access Machine (RAM) model</h2>

<blockquote>
  <p>To analyse programs, we use a <strong>simplified model</strong> of how computers work to <strong>help</strong> think about the time an high level operation takes to run by expressing it as fundamental operations which are equivocal to real computers.</p>
</blockquote>

<p>In the RAM model, we consider a computer with:</p>
<ul>
  <li>A single CPU executing a single program</li>
  <li>An arbitrarily large indexable array of memory</li>
  <li>A set of registers memory can be copied into</li>
  <li>Basic arithmetic and memory allocation operations</li>
</ul>

<p>Generally, we tend to abstract beyond this model to just consider a set of <strong>primitive operations</strong> (usually single lines of pseudocode) that take constant time <strong>irrespective</strong> of input size in the RAM model.</p>

<blockquote>
  <p>We then <strong>analyse performance</strong> by <strong>counting</strong> the number of operations needed, as their number is proportional to running time.</p>
</blockquote>

<p>This allows us to express the running time of the program as being between the best and worst cases of number of operations needed, multiplied their running time</p>
<ul>
  <li>Let \(T(n)\) denote the running time, \(b(n)\) the best case, \(w(n)\) the worst case, and \(t\) the time taken for 1 primitive operation</li>
  <li>The running time is bounded as \(t \times b(n) \leq T(n) \leq t \times w(n)\)</li>
  <li>This metric of running time \(T(n)\) is <strong>not dependent</strong> on machine hardware or software environment – it is an <strong>intrinsic property</strong> of the algorithm.</li>
</ul>

<h2 id="big-o-notation">Big-O Notation</h2>

<blockquote>
  <p><strong>Big-O</strong> is a way of quantifying the running time of an algorithm, allowing easy comparison. Given the functions \(f(n)\) and \(g(n)\), we say that \(f(n)\) is \(O(g(n))\) if:</p>

\[\begin{align}
&amp;f(n) \leq g(n) \cdot c,&amp; &amp;\text{for all } n \geq n_0,&amp; \\
&amp;&amp; &amp;\text{with some positive} \\ 
&amp;&amp; &amp;\text{constants } c \text{ and } n_0
\end{align}\]
</blockquote>

<p>Informally, this means that \(f(n)\) is “overtaken” by \(g(n)\) for all values above some threshold \(n _0\) usually we consider \(n \rightarrow \infty\), <strong>allowing scaling</strong> by a linear factor \(c\). This can be phrased as “\(f(n)\) is \(O(g(n))\) if \(g(n)\) grows as fast or faster than \(f(n)\) in the limit of \(n \rightarrow \infty\)” (<a href="https://math.stackexchange.com/questions/620145/understanding-definition-of-big-o-notation/620150#620150">Source</a>)</p>

<p>Big-O notation, thus, <strong>gives an upper bound</strong> on the growth rate of a function as its input size <em>n</em> tends to infinity. Hence, \(f(n)\) is \(O(g(n))\) means that the <strong>growth rate of \(f(n)\)</strong> is <strong>no greater</strong> than that of the <strong>growth rate of \(g(n)\)</strong>.</p>

<h3 id="big-o-of-a-function">Big-O of a Function</h3>

<p>Informally, the Big-O of a function is the term that grows the fastest, as it will come to <strong>dominate</strong> for a very large <em>n</em>, and we then just pick <em>n<sub>0</sub></em> where that term is dominating, and use <em>c</em> to shift the function to fit.</p>

<blockquote>
  <p>So, if \(f(n)\) is a polynomial of degree \(d\), then \(f(n)\) is \(O(n^d)\), as we can drop all <strong>but</strong> the fastest growing term.</p>
</blockquote>

<p>When writing Big-O, we:</p>

<ul>
  <li>Try to use the smallest possible class of functions which fulfils the criteria.
    <ul>
      <li>E.g. <em>O(n)</em> not <em>O(n<sup>2</sup>)</em>, whilst both technically are Big-O of linear functions. (<a href="https://cs.stackexchange.com/questions/77653/why-the-big-oh-of-a-linear-function-is-n2">Why is O(n<sup>2</sup>) valid for linear functions?</a>)</li>
    </ul>
  </li>
  <li>Use the simplest expression of the class.
    <ul>
      <li>E.g. <em>O(n)</em> not <em>O(5n)</em>.</li>
    </ul>
  </li>
</ul>

<h2 id="asymptotic-algorithm-analysis">Asymptotic Algorithm Analysis</h2>

<blockquote>
  <p><strong>Asymptotic algorithm analysis</strong> is a way we can take pseudocode and use it to find the Big-O of an algorithm.</p>

  <ul>
    <li>We first consider the <strong>worst-case</strong> number of primitive operations that the algorithm could require to run as a function of its input size.</li>
    <li>We then express this derived function in <strong>Big-O notation</strong>.</li>
  </ul>
</blockquote>

<p>To prove something is \(O(f(n))\), we need to show that we can pick a \(c\) and an \(n\) which satisfy the condition. To prove something is <strong>not</strong> ,\(O(f(n))\) we show that there is <strong>no</strong> \(c\) for any arbitrarily large \(n_0\) which satisfies the condition.</p>

<p><strong>An example,</strong> of this being formally calculated (taken from <em>Data Structures and Algorithms in Java, Goodrich, Tamassia, Goldwasser</em>) is shown below:</p>

<blockquote>
  <p>Consider the function \(2n + 10\). To show that it is \(O(n)\), we take:</p>

\[\begin{align}
 2n + 10 &amp;\le c \cdot n \\
 cn-2n &amp;\ge 10   \\
 n &amp;\ge \frac{10}{c-2}
 \end{align}\]

  <p>Hence, picking <em>c = 3</em> and <em>n<sub>0</sub> = 10</em> the condition is <strong>satisfied</strong>.</p>
</blockquote>

<p><img src="./images/bigOh.png" alt="bigOh" /></p>

<p>Image source: <em>Data Structures and Algorithms in Java, Goodrich, Tamassia, Goldwasser</em></p>

<h2 id="relatives-of-big-o">Relatives of Big-O</h2>

<p>There are other “relatives” of Big-O</p>

<ul>
  <li>Big-O gives the upper bound
    <ul>
      <li>\(f(n)\le g(n)\) in the limit of \(n\rightarrow \infty\)</li>
    </ul>
  </li>
  <li>Big-Ω gives the lower bound
    <ul>
      <li>\(f(n)\ge g(n)\) in the limit of \(n\rightarrow \infty\)</li>
    </ul>
  </li>
  <li>Big-Θ gives “asymptotically tight” ≈ average
    <ul>
      <li>\(f(n)= g(n)\) in the limit of \(n\rightarrow \infty\)</li>
    </ul>
  </li>
</ul>

<p><a href="https://courses.cs.washington.edu/courses/cse326/06au/lectures/lect03.pdf">Additional notes</a></p>
<h1 id="recursive-algorithms">Recursive algorithms</h1>
       
        <h2 id="definition">Definition</h2>

<p>Recursion can be defined in various ways:</p>

<blockquote>
  <p>“When a method calls itself”</p>

  <p>– <em>Data Structures and Algorithms in Java, Goodrich, Tamassia, Goldwasser</em></p>

  <p>“A method which is expressed in terms of calls to simpler cases of itself, and a base case”</p>

  <p>– <em>CSRG, Edmund Goodman</em> (It’s a recursive reference, get it?) 🙃</p>
</blockquote>

<h2 id="structure">Structure</h2>

<p>Recursive functions tend to include two main components:</p>

<ol>
  <li>Base cases</li>
  <li>Recursive calls</li>
</ol>

<p><strong>Base cases</strong> tend to be simple input values where the return value is a known constant, so no recursive calls are made. They are <strong>required</strong> for a recursive function <strong>to finish evaluating</strong>, so there must be <strong>at least</strong> one</p>

<p><strong>Recursive calls</strong> are ones to the same recursive function making the call, with a simpler input (since it must “<strong>move towards</strong>” the base case for it to ever finish evaluating)</p>

<p>We can visualise recursion by drawing diagrams of functions, with functions as boxes and arrows indicating calls and return values. This is fairly self-explanatory.</p>

<h2 id="examples">Examples</h2>

<p>We can often express many functions both iteratively and recursively, such as a <strong>binary search</strong>, which can be implemented recursively with:</p>
<ul>
  <li>The input being a list,</li>
  <li>The recursive call being the half of the list the search has been narrowed down to</li>
  <li>The base cases being a single item, returning the index of that item if it is the item being searched for, or an indicator of absence if not</li>
</ul>

<p>See the page on general algorithms for the <a href="https://csrg-group.github.io/dcs-notes.github.io/CS126/part12.html#recursive-algorithm">pseudocode for a recursive binary search</a></p>

<h2 id="types-of-recursion">Types of recursion</h2>

<p><strong>Linear recursion.</strong> Each functional call makes only one recursive call (there may be multiple different possible calls, but only one is selected), or none if it is a base case.</p>

<p><strong>Binary and multiple recursion.</strong> Each functional call makes two or multiple recursive calls, unless it is a base case.</p>
<h1 id="stacks-and-queues">Stacks and Queues</h1>
       
        <h2 id="stacks-adt">Stacks (ADT)</h2>
<blockquote>
  <p><strong>Stacks</strong> are a “Last in, first out” (LIFO) data structure, with both insertions and deletions always occurring at the front of the stack.</p>
</blockquote>

<p>These insertions and deletions are the fundamental operations of the stack, called pushing and popping respectively.</p>

<p>There is an edge case of popping from an empty stack, which normally either returns null or throws an error</p>

<p>Stacks have the fundamental operations:</p>

<ul>
  <li>push(e)</li>
  <li>pop()</li>
  <li>size()</li>
  <li>isEmpty()</li>
</ul>

<h3 id="array-based-implementation">Array Based Implementation</h3>

<p>Index of head stored, and incremented/decremented on pushing/popping operations</p>

<ul>
  <li>
    <p><em>O(n)</em> space complexity</p>
  </li>
  <li>
    <p><em>O(1)</em> time complexity of fundamental operations</p>
  </li>
</ul>

<h2 id="queues-adt">Queues (ADT)</h2>

<blockquote>
  <p><strong>Queues</strong> are a “First in, first out” (FIFO) data structure, with insertions occurring at the rear and removals at the front of the queue.</p>
</blockquote>

<p>These insertions and deletions are the fundamental operations of the stack, called enqueueing and dequeuing respectively.</p>

<p>There is an edge case of dequeuing from an empty queue, normally either returns null or throws an error</p>

<p>Queues have the fundamental operations</p>

<ul>
  <li>enqueue(e)</li>
  <li>dequeue()</li>
  <li>size()</li>
  <li>isEmpty()</li>
</ul>

<h3 id="array-based-implementation-1">Array Based Implementation</h3>

<p>Uses and array with data wrapping (so like using an array in a <code class="language-plaintext highlighter-rouge">Queue</code> class with extra fields/properties) around as it is added and removed. Both the index of the head <strong><em>f</em></strong> <strong>and</strong> the size of the queue <strong><em>s</em></strong> need to be stored.</p>

<p>The rear of the queue (index to insert to next) is <em>(f + s)</em> mod <em>N</em>, with <em>N</em> as the array size</p>

<p><img src="./images/queueArrayImplementation.png" alt="queueArrayImplementation" /></p>

<p><em>Image source: Data Structures and Algorithms in Java, Goodrich, Tamassia, Goldwasser</em></p>

<ul>
  <li><em>O(n)</em> space complexity</li>
  <li><em>O(1)</em> time complexity of fundamental operations</li>
</ul>
<h1 id="maps-hash-tables-and-sets">Maps, Hash tables and Sets</h1>
       
        <h2 id="maps-adt">Maps (ADT)</h2>
<blockquote>
  <p><strong>Maps</strong> are a “searchable collection of key-value entries”</p>

  <p><em>Data Structures and Algorithms in Java, Goodrich, Tamassia, Goldwasser.</em></p>
</blockquote>

<p>They cannot contain duplicate keys, as then they would not be able to unambiguously look up values by their keys</p>

<p>Maps have the fundamental operations:</p>
<ul>
  <li>contains(e)</li>
  <li>get(e)</li>
  <li>put(e)</li>
  <li>remove(e)</li>
  <li>size()</li>
  <li>isEmpty()</li>
  <li><em>sometimes additional operations for getting lists of all keys or all values</em></li>
</ul>

<p>There are two common concrete implementations:</p>
<ul>
  <li>List based implementation
    <ul>
      <li>\(O(n)\) lookup and insertion, as the whole list needs to be iterated over to check for duplicates</li>
      <li>\(O(n)\) space complexity, as there are no duplicates</li>
    </ul>
  </li>
  <li>Hash table based implementation
    <ul>
      <li>\(O(1)\) lookup and insertion time, as they can be directly indexed</li>
      <li>\(O(k \cdot n)\) space complexity (still linear with number of items, but larger by a big constant factor)</li>
    </ul>
  </li>
</ul>

<h2 id="hash-tables">Hash tables</h2>

<p><em>Concrete implementation</em></p>

<blockquote>
  <p><strong>Hash tables</strong> are a time efficient implementation of the Map abstract data type</p>
</blockquote>

<p>To look up keys in \(O(1)\) time, we want essentially want to be able to index an array of them, but the space of keys are far too large to conceivably keep <strong>just</strong> one element in the array for each key.</p>

<h3 id="hash-functions">Hash functions</h3>

<p>We can use a “hash function” to reduce the size of the keyspace, so we can used the hashed outputs of keys for indices in the array storing the map.
\(h : keys \rightarrow indices\)
\(h\) maps keys of a given type to integers in a fixed interval \([0, N-1]\) where \(N\) is the size of the array to store the items in.</p>

<p>Modern implementations of hash functions are <strong>very complicated</strong>, and often involve two phases</p>

<ol>
  <li>Mapping keys to integers with a <strong>hash code</strong> \(h_1\)</li>
  <li>Reducing the range of those integers with a <strong>compression function</strong> \(h_2\)</li>
</ol>

<p>But simpler ones exist, for example \(h(x) =  x \!\!\mod \!N\)</p>

<ul>
  <li>We try to pick \(N\) such that there are fewer collisions – numbers like primes with few factors are better</li>
</ul>

<h3 id="memory-address">Memory address</h3>

<p>Java implements hash functions for all objects with the <code class="language-plaintext highlighter-rouge">.hashCode()</code> method, giving a convenient way to implement hashing.</p>

<p>The <code class="language-plaintext highlighter-rouge">.hashCode()</code> method is dependent on the memory address of the object storing the key, which is then cast to an integer. This then may be resized using a reduction function to map it to the correct size of the table may still be required.</p>

<h3 id="integer-cast">Integer cast</h3>

<p>Taking the bits encoding the object storing the key, and re-interpreting them as an integer. This is only suitable for keys of fewer or equal to the number of bits in the integer type (i.e. primitives: <code class="language-plaintext highlighter-rouge">byte</code>, <code class="language-plaintext highlighter-rouge">short</code>, <code class="language-plaintext highlighter-rouge">int</code>, <code class="language-plaintext highlighter-rouge">float</code>)</p>

<h3 id="component-sum">Component sum</h3>

<p>The process is:</p>

<ol>
  <li>Partition the bits of the key into a number of fixed length components (e.g. 8 bits)</li>
  <li>Sum together the components, discarding overflows</li>
</ol>

<p>This is suitable for keys of a greater number of bits than the integer type (e.g. <code class="language-plaintext highlighter-rouge">long</code> and <code class="language-plaintext highlighter-rouge">double</code>)</p>

<h3 id="polynomial-accumulation">Polynomial accumulation</h3>

<p>The process is:</p>

<ol>
  <li>
    <p>Partition the bits of the key into a number of fixed length components (e.g. 8 bits), and name them \(a_0, a_1, ..., a_{n-1}\) respectively</p>
  </li>
  <li>
    <p>Evaluate the polynomial:
\(p(z) = a_0 + a_1 \cdot z + a_2 \cdot z_2 + ... + a_{n-1} \cdot z^{n-1}\)
at a fixed value \(z\), ignoring overflows</p>

    <p>This can be evaluated quickly using Horner’s rule</p>
  </li>
</ol>

<p>This is especially suitable for strings, with \(z=33\) giving at most \(6\) collisions from \(50,000\) English words</p>

<h3 id="java-hash-implementations">Java hash implementations</h3>

<p>Java implements hash functions for all objects with the <code class="language-plaintext highlighter-rouge">.hashCode()</code> method, giving a convenient way to implement hashing, but a reduction function to map it to the correct size of the table may still be required.</p>

<p>Additionally: “You must override <code class="language-plaintext highlighter-rouge">hashCode()</code> in every class that overrides <code class="language-plaintext highlighter-rouge">equals()</code>.  Failure to do so will result in a violation of the general contract for <code class="language-plaintext highlighter-rouge">Object.hashCode()</code>, which will prevent your class from functioning  properly in conjunction with all hash-based collections, including  <code class="language-plaintext highlighter-rouge">HashMap</code>, <code class="language-plaintext highlighter-rouge">HashSet</code>, and <code class="language-plaintext highlighter-rouge">Hashtable</code>.” (<em>Effective Java</em>, Joshua Bloch)</p>

<h2 id="collisions">Collisions</h2>

<blockquote>
  <p>Collisions are when two <strong>different</strong> keys are <strong>mapped to</strong> the <strong>same index</strong> by the hash function. Since we cannot store duplicate keys unambiguously in a map, we need a protocol to resolve this.</p>
</blockquote>

<p>Common approaches to this are</p>

<ul>
  <li>Separate chaining</li>
  <li>Linear probing</li>
  <li>Double hashing</li>
</ul>

<p>When colliding items are placed in different cells in the table, it is called “open addressing”, and when they are put in a separate data structure it is called closed addressing (with linear probing and separate chaining being examples of both respectively)</p>

<h3 id="separate-chaining">Separate Chaining</h3>

<p>In <strong>separate chaining</strong>, each index in the array can contain a reference to a linked list.</p>

<ul>
  <li>Whenever a key is mapped to that index, the key-value pair is added to the linked-list.</li>
  <li>If there are duplicates, we iterate over the chain till we find the key, or reach the end.</li>
</ul>

<p>This has the <strong>drawback</strong> of requiring additional memory space for each linked list</p>

<p><img src="./images/separateChaining.png" alt="separateChaining" /></p>

<p><em>Image source: Data Structures and Algorithms in Java, Goodrich, Tamassia, Goldwasser</em></p>

<h3 id="linear-probing">Linear Probing</h3>

<blockquote>
  <p><strong>Linear probing</strong> handles collisions by placing the colliding item in the next available table cell, wrapping around if necessary.</p>
</blockquote>

<p><strong>Searching</strong></p>

<p>As with the linked list, <strong>searching</strong> is done by iterating over the next cells, stopping when</p>

<ul>
  <li>The item is found</li>
  <li>An empty cell in the table is found</li>
  <li><strong><em>N</em></strong> cells have been unsuccessfully (cannot find key) probed.</li>
</ul>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// Psuedocode</span>
<span class="n">Algorithm</span> <span class="n">get</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
    <span class="n">i</span> <span class="o">&lt;-</span> <span class="n">h</span><span class="p">(</span><span class="n">k</span><span class="p">)</span> <span class="c1">// h = hash function</span>
    <span class="n">p</span> <span class="o">&lt;-</span> <span class="mi">0</span>
    <span class="n">repeat</span>
    	<span class="n">c</span> <span class="o">&lt;-</span> <span class="n">A</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="c1">// A is the table</span>
    	<span class="k">if</span> <span class="n">c</span> <span class="o">=</span> <span class="n">empty</span>
            <span class="k">return</span> <span class="n">null</span>
        <span class="k">else</span> <span class="k">if</span> <span class="n">c</span><span class="p">.</span><span class="n">getKey</span><span class="p">()</span> <span class="o">=</span> <span class="n">k</span> <span class="c1">// We found our item</span>
            <span class="k">return</span> <span class="n">c</span><span class="p">.</span><span class="n">getValue</span><span class="p">()</span>
        <span class="k">else</span>
            <span class="n">i</span> <span class="o">&lt;-</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="n">mod</span> <span class="n">N</span> <span class="c1">// mod N takes care of wrap arounds</span>
            <span class="n">p</span> <span class="o">&lt;-</span> <span class="n">p</span> <span class="o">+</span> <span class="mi">1</span>
   <span class="n">until</span> <span class="n">p</span> <span class="o">=</span> <span class="n">N</span> <span class="c1">// stop if we have repeated N times</span>
   <span class="k">return</span> <span class="n">null</span>
</code></pre></div></div>

<p>This has the <strong>drawback</strong> of colliding items “lumping together”, which can cause many items needed to be iterated over in a probe.</p>

<p><strong>Removing</strong></p>

<p><strong>To remove</strong> an item, we cannot just set it to null again, as that would mean it stops probing, even though there might be subsequent elements. Instead, we replace it with a <code class="language-plaintext highlighter-rouge">DEFUNCT</code> element, which is just skipped over when probing.</p>

<ul>
  <li>Search for an entry with key <strong><em>k</em></strong></li>
  <li>If <strong><em>k</em></strong> is found, we replace it with <code class="language-plaintext highlighter-rouge">DEFUNCT</code> and we return the <strong>value</strong> of the item with key <strong><em>k</em></strong></li>
  <li>Else we return <strong>null</strong></li>
</ul>

<p>When colliding items are placed in different cells in the table, it is called <strong>open addressing</strong>, or <strong>closed hashing</strong>, and when they are put in a separate data structure it is called <strong>closed addressing</strong>, or <strong>separate chaining</strong> (with linear probing and separate chaining being examples of both respectively) <a href="http://www.iro.umontreal.ca/~nie/IFT1020/Watt/12/tsld009.htm">additional link</a></p>

<h3 id="double-hashing">Double Hashing</h3>

<blockquote>
  <p><strong>Double hashing</strong> handles collisions by re-hashing the key with a new hash function</p>

  <p>If cell \(h(k)\) is occupied and not our key, we try \([h(k) + i \cdot f(k)] \!\!\mod \!N, \; i \in \mathbb{Z}\)</p>

  <ul>
    <li>\(h\) and \(f\) are hashing functions, and \(f(k)\) cannot have 0 values.</li>
    <li>\(N\) must be a prime to allow probing of all cells.</li>
  </ul>
</blockquote>

<p>As before, there are many implementations of the hash function, but \(f(k)= q-k \!\!\mod\!q, \;s.t.\;  q&lt;N, q \in primes\) is normally used.</p>

<ul>
  <li>If \(f(k) = 1\) then we have <strong>linear probing</strong>.</li>
</ul>

<p><strong>Searching</strong> is similar to linear probing, but when iterating we look at the hash value for \(i = 1,2,3,\ldots\) rather than just the next index in the table. This helps avoid the issue of colliding items “lumping together” as in linear probing.</p>

<h2 id="resizing-a-hash-table">Resizing a hash table</h2>

<p>As with arrays, we create a new table of a larger size, then iterate over every index in the table, and apply the standard add operation to add it to the new one (re-hashing).</p>

<p>Again, similarly to arrays, the new size of the table can be picked from various algorithms, most commonly constant or exponential growth.</p>

<h2 id="performance-of-hashing">Performance of Hashing</h2>

<p>The load factor of a hash table is the ratio of the number of items it contains to the capacity of the array \(\alpha = \frac{n}{N}\).</p>

<ul>
  <li>If this approaches \(1\), the table becomes time inefficient to lookup in, so we often re-size the table whenever it exceeds a certain value, e.g. \(0.75\)</li>
  <li>If this approaches \(0\), then the table is mostly empty, so is space inefficient, so we try to avoid tables of less than a certain value, e.g. \(0.5\)</li>
</ul>

<p>The time complexity of insertion and lookup is:</p>
<ul>
  <li>\(\Theta(1)\) best case</li>
  <li>\(O(n)\) worst case – when all keys inserted into the map collide</li>
  <li>“Expected” number of probes with open addressing is \(\frac{1}{1-\alpha}\)</li>
</ul>

<p>In practice, hash tables are a very efficient implementation of maps assuming the load factor is not very close to \(1\)</p>

<blockquote class="extra">
    Experiments show that as long as \(\alpha \lt 0.9\), there should be no problem with speed.
    However, for \(\alpha \gt 0.9\) the number of collisions increase and becomes slower.
</blockquote>

<h2 id="sets-adt">Sets (ADT)</h2>

<blockquote>
  <p><strong>Sets</strong> are “an <strong>unordered</strong> collection of elements, <strong>without duplicates</strong> that typically supports <strong>efficient membership tests</strong>.”</p>

  <p><em>Data Structures and Algorithms in Java, Goodrich, Tamassia, Goldwasser</em></p>
</blockquote>

<p>These are the same as sets in mathematics.</p>

<p><em>If you want to pull request more stuff here, please do - but I’m not too sure how much more depth is needed</em></p>

<table>
  <thead>
    <tr>
      <th>Fundamental Operations</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">add(e)</code></td>
      <td>Adds the element <em>e</em> to <em>S</em> (if not already present)</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">remove(e)</code></td>
      <td>Removes the element <em>e</em> from <em>S</em> (if it is present).</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">contains(e)</code></td>
      <td>Returns whether <em>e</em> is an element of <em>S</em></td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">iterator()</code></td>
      <td>Returns an iterator of the elements of <em>S</em></td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">union(s2)</code></td>
      <td>Updates <em>S</em> to also include all elements of set <em>T</em>. This effectively replaces <em>S</em> with <em>S</em> ∪ <em>T</em></td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">intersection(s2)</code></td>
      <td>Replace/Update <em>S</em> with <em>S</em> ∩ <em>T</em></td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">difference(s2)</code></td>
      <td>Replace/Update <em>S</em> with <em>S</em> – <em>T</em>  (set minus)</td>
    </tr>
  </tbody>
</table>

<p>And alternate definition for set operations can instead define a third set structure and fill it with the result of <em>S</em> *set operation* <em>T</em> – this way we don’t alter <em>S</em></p>

<ul>
  <li>union(s1, s2)</li>
  <li>intersection(s1, s2)</li>
  <li>difference(s1, s2)</li>
</ul>

<h2 id="implementations">Implementations</h2>

<p>There are two common concrete implementations. These are essentially the same as for maps, however, the key and the value are taken to be the same.</p>

<ul>
  <li>Linked lists</li>
  <li>Hash set</li>
</ul>

<h3 id="list-based">List based</h3>

<blockquote>
  <p>In the list implementation we store elements <strong>sorted</strong> according to some canonical ordering. This is <strong>important</strong> for the set operations to be more time efficient.</p>
</blockquote>

<p>Generally, the <strong>space complexity</strong> is \(O(n)\), without overhead of empty cells. Since sets are not indexable, linked lists can be used, <strong>offering efficient re-sizing</strong>.</p>

<p>We need to iterate over each element in the list to lookup items, \(O(n)\) time complexity, which is not efficient, but for most more complex set operations, this becomes less of a drawback.</p>

<h4 id="generic-merging-algorithm">Generic Merging Algorithm</h4>

<p>Set operations can be implemented using a generic <strong>merge</strong> algorithm.</p>

<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Algorithm</span> <span class="nf">genericMerge</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">B</span><span class="p">)</span>
    <span class="n">S</span> <span class="o">&lt;-</span> <span class="n">empty</span> <span class="n">set</span> <span class="c1">// Set S to be an empty set</span>
    <span class="k">while</span> <span class="o">!</span><span class="n">A</span><span class="p">.</span><span class="n">isEmpty</span><span class="p">()</span> <span class="n">and</span> <span class="o">!</span><span class="n">B</span><span class="p">.</span><span class="n">isEmpty</span><span class="p">()</span> <span class="c1">// until either 1 is empty</span>
        <span class="n">a</span> <span class="o">&lt;-</span> <span class="n">A</span><span class="p">.</span><span class="n">first</span><span class="p">().</span><span class="n">element</span><span class="p">();</span> <span class="n">b</span> <span class="o">&lt;-</span>  <span class="n">B</span><span class="p">.</span><span class="n">first</span><span class="p">().</span><span class="n">element</span>
        <span class="k">if</span> <span class="n">a</span> <span class="o">&lt;</span> <span class="n">b</span>
            <span class="nf">aIsLess</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">S</span><span class="p">);</span> <span class="n">A</span><span class="p">.</span><span class="n">remove</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">first</span><span class="p">())</span>
        <span class="k">else</span> <span class="k">if</span> <span class="n">b</span> <span class="o">&lt;</span> <span class="n">a</span>
            <span class="nf">bIsLess</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">S</span><span class="p">);</span> <span class="n">B</span><span class="p">.</span><span class="n">remove</span><span class="p">(</span><span class="n">B</span><span class="p">.</span><span class="n">first</span><span class="p">())</span>
        <span class="k">else</span> <span class="c1">// b == a</span>
            <span class="n">bothAreEqual</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">S</span><span class="p">)</span>
            <span class="n">A</span><span class="p">.</span><span class="n">remove</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">first</span><span class="p">());</span> <span class="n">B</span><span class="p">.</span><span class="n">remove</span><span class="p">(</span><span class="n">B</span><span class="p">.</span><span class="n">first</span><span class="p">())</span>
    <span class="c1">// By this point either A is empty or B is empty</span>
    <span class="k">while</span> <span class="o">!</span><span class="n">A</span><span class="p">.</span><span class="n">isEmpty</span><span class="p">()</span> <span class="c1">// populate S with remaining elements in A, if any</span>
        <span class="n">aIsLess</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">S</span><span class="p">);</span> <span class="n">A</span><span class="p">.</span><span class="n">remove</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="n">first</span><span class="p">())</span>
    <span class="k">while</span> <span class="o">!</span><span class="n">B</span><span class="p">.</span><span class="n">isEmpty</span><span class="p">()</span> <span class="c1">// Same for B as with A</span>
        <span class="n">bIsLess</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">S</span><span class="p">);</span> <span class="n">B</span><span class="p">.</span><span class="n">remove</span><span class="p">(</span><span class="n">B</span><span class="p">.</span><span class="n">first</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">S</span>
</code></pre></div></div>

<p>❕❗ This merging algorithm is used in <strong>merge sort</strong> as well! You may have noticed that we have 3 auxiliary methods in this algorithm: <code class="language-plaintext highlighter-rouge">aIsLess</code>, <code class="language-plaintext highlighter-rouge">bIsLess</code>, and <code class="language-plaintext highlighter-rouge">bothAreEqual</code>.</p>

<blockquote>
  <p>Depending on the set operation (or any operation you are using this generic merge for), you define these methods <strong>differently</strong>.</p>

</blockquote>

<p><strong>Example.</strong></p>

<p>For <strong>set intersection</strong> – we only want the algorithm to merge when <code class="language-plaintext highlighter-rouge">b == a</code>, so <code class="language-plaintext highlighter-rouge">aIsLess</code> and <code class="language-plaintext highlighter-rouge">bIsLess</code> should do <strong>nothing</strong> and <code class="language-plaintext highlighter-rouge">bothAreEqual</code> should add either one into <strong><em>S</em></strong>.</p>

<p><strong>Set union</strong> is trivial (just add everything).</p>

<p>For <strong>set subtraction</strong> you do nothing if the elements are equal!</p>

<p>This means that each <strong>set operation</strong> runs in <strong>linear time</strong> (i.e \(O(n_A + n_B)\) time), provided that the auxiliary methods run in <em>O(1)</em> time. This is <strong>possible</strong>, as we know that the <strong>elements are sorted</strong>.</p>

<h3 id="hash-set-based">Hash-set based</h3>

<p>Hash-sets are implemented like a hash-table, but using only keys, not key-value pairs. This gives fast \(O(1)\) lookups, and an \(O(n)\) space complexity, but with large overheads.</p>
<h1 id="trees">Trees</h1>
       
        <h2 id="trees-adt">Trees (ADT)</h2>
<blockquote>
  <p><strong>Trees</strong> are “an abstract model of a hierarchical structure. A tree consists of nodes with a parent-child relation.”</p>

  <p><em>Data Structures and Algorithms in Java, Goodrich, Tamassia, Goldwasser</em></p>
</blockquote>

<p><em>If you want to pull request more stuff here, please do - but I’m not too sure how much more depth is needed</em></p>

<table>
  <thead>
    <tr>
      <th>Fundamental Operation</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">size()</code></td>
      <td>Size of tree – number of nodes</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">isEmpty()</code></td>
      <td>Returns <code class="language-plaintext highlighter-rouge">true</code> if the tree is empty.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">iterator()</code></td>
      <td>Iterator for the tree</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">positions()</code></td>
      <td>Return an iterable container of all nodes in the tree</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">root()</code></td>
      <td>Returns root node</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">parent(p)</code></td>
      <td>Returns parent of node <strong><em>p</em></strong></td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">children(p)</code></td>
      <td>Returns an iterable container of the children of node <strong><em>p</em></strong></td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">numChildren(p)</code></td>
      <td>Returns the number of children of node <strong><em>p</em></strong></td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">isInternal(p)</code></td>
      <td>Returns <code class="language-plaintext highlighter-rouge">true</code> if node <strong><em>p</em></strong> is an <strong>internal node</strong> (node with at least 1 child)</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">isExternal(p)</code></td>
      <td>Returns <code class="language-plaintext highlighter-rouge">true</code> if node <strong><em>p</em></strong> is an <strong>external node</strong> (node with no children)</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">isRoot(p)</code></td>
      <td>Returns <code class="language-plaintext highlighter-rouge">true</code> if node <strong><em>p</em></strong> is a <strong>root node</strong> (node without parent)</td>
    </tr>
  </tbody>
</table>

<p>The methods for insertion, deletion, and searching are more complicated, and so are outlined in more detail in the binary search tree section</p>

<h2 id="tree-traversals">Tree Traversals</h2>

<p>There are various ways a tree can be traversed. Shown here is a figure of a binary tree.</p>

<p><img src="https://mermaid.ink/svg/eyJjb2RlIjoiZ3JhcGggVEQ7XG5cdEEgLS0tIEI7XG5cdEEgLS0tIEM7XG5cdEIgLS0tIEQ7XG5cdEIgLS0tIEU7IFxuXHRDIC0tLSBGO1xuXHRDIC0tLSBHO1xuIiwibWVybWFpZCI6eyJ0aGVtZSI6ImRlZmF1bHQifSwidXBkYXRlRWRpdG9yIjpmYWxzZX0" class="center" /></p>

<p><strong>In-order (Left, Root, Right).</strong> DBE A FCG</p>

<p><strong>Pre-order (Root, Left, Right).</strong> A BDE CFG</p>

<p><strong>Post-order (Left, Right, Root).</strong> DEB FGC A</p>

<p><strong>Breadth First/Level Order.</strong> ABCDEFG</p>

<p>We will come back to breadth first traversal in a later topic (Breadth First Search). For now we will focus on the first 3.</p>

<h3 id="in-order-traversal">In-Order Traversal</h3>

<p>For every node, we print the left child, the node itself, then the right child. Since this is a recursive function, if we start at a node <strong><em>n</em></strong>, the algorithm will start from the left-most child <strong>of the tree</strong>, then that child’s parent then its sibling and on for the entire tree that the <strong><em>n</em></strong> is the <strong>root</strong> of.</p>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">Function</span> <span class="nf">inOrder</span><span class="o">(</span><span class="n">n</span><span class="o">)</span>
  <span class="k">if</span> <span class="n">n</span> <span class="o">!=</span> <span class="kc">null</span>
    <span class="nf">inOrder</span><span class="o">(</span><span class="n">n</span><span class="o">.</span><span class="na">leftChild</span><span class="o">())</span>
    <span class="nc">Print</span> <span class="n">n</span>
    <span class="nf">inOrder</span><span class="o">(</span><span class="n">n</span><span class="o">.</span><span class="na">rightChild</span><span class="o">())</span>
</code></pre></div></div>

<p>Note that the above algorithm applies only to <strong>binary trees</strong>, for a more general form of in-order traversal, there will need to be an <strong>additional</strong> definition of what makes a node a “left child”. This can either be that left child nodes have a smaller value than the parent/root, or left children are just the first <strong><em>m</em></strong> number of nodes etc.</p>

<h3 id="pre-order-traversal">Pre-order traversal</h3>

<p>Each node is printed before its descendants, and descendants are taking in ascending order</p>
<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">Function</span> <span class="nf">preOrder</span><span class="o">(</span><span class="n">n</span><span class="o">)</span>
  <span class="k">if</span> <span class="n">n</span> <span class="o">!=</span> <span class="kc">null</span>
    <span class="nc">Print</span> <span class="n">n</span>
    <span class="nc">For</span> <span class="n">each</span> <span class="n">child</span> <span class="n">m</span> <span class="n">of</span> <span class="n">n</span>
      <span class="nf">preOrder</span><span class="o">(</span><span class="n">n</span><span class="o">)</span>
</code></pre></div></div>

<h3 id="post-order-traversal">Post-order traversal</h3>

<p>Each node is printed after its descendants, and descendants are taking in ascending order</p>
<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">Function</span> <span class="nf">postOrder</span><span class="o">(</span><span class="n">n</span><span class="o">)</span>
  <span class="k">if</span> <span class="n">n</span> <span class="o">!=</span> <span class="kc">null</span>
    <span class="nc">For</span> <span class="n">each</span> <span class="n">child</span> <span class="n">m</span> <span class="n">of</span> <span class="n">n</span>
      <span class="nf">postOrder</span><span class="o">(</span><span class="n">n</span><span class="o">)</span>
    <span class="nc">Print</span> <span class="n">n</span>
</code></pre></div></div>

<h2 id="binary-trees-adt">Binary trees (ADT)</h2>

<blockquote>
  <p><strong>Binary trees</strong> are a specialised tree where each node has at most two children, called left and right</p>
</blockquote>

<table>
  <thead>
    <tr>
      <th>Fundamental Operations</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">left(p)</code></td>
      <td>Returns left child of node <strong><em>p</em></strong></td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">right(p)</code></td>
      <td>Returns right child of node <strong><em>p</em></strong></td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">sibling(p)</code></td>
      <td>Returns sibling of node <strong><em>p</em></strong></td>
    </tr>
  </tbody>
</table>

<h3 id="properties">Properties</h3>

<p>A binary tree with \(n\) nodes, \(e\) external nodes, \(i\) internal nodes, and a height \(h\) has the properties</p>

<ol>
  <li>
\[e = i + 1\]
  </li>
  <li>
\[n = 2e - 1\]
  </li>
  <li>
\[h \leq i\]
  </li>
  <li>
\[h \leq \frac{(n-1)}{2}\]
  </li>
  <li>
\[e \leq 2^h\]
  </li>
  <li>
\[h \geq log_2 e\]
  </li>
  <li>
\[h \geq log_2 (n+1) - 1\]
  </li>
</ol>

<p>As mentioned <a href="#in-order-traversal">earlier</a>, Binary Trees by definition have a <strong>discrete middle node</strong>, and inherently support <strong>in-order traversal</strong>.</p>

<h3 id="implementations">Implementations</h3>

<p>There are two common concrete implementations of binary trees</p>

<ul>
  <li>Linked structure</li>
  <li>Array based</li>
</ul>

<h3 id="linked-structure">Linked structure</h3>

<p>In the linked structure implementation, each node is an object which stores its value, references to its child nodes (and sometimes a reference to its parent), as shown in the diagram below:</p>

<p><img src="./images/binaryTreeLinkedStructure.png" alt="binaryTreeLinkedStructure" class="center" /></p>

<p>Image source: <em>Data Structures and Algorithms in Java, Goodrich, Tamassia, Goldwasser</em></p>

<p>This has a linear space complexity irrespective of the balance of the tree, and has a lookup time of \(O(log_2n)\) for lookup operations.</p>

<h3 id="array-based">Array based</h3>

<p>In the array based implementation, node values are stored in an array, and their children can be found at indices based on arithmetic operations of their own index</p>

<ul>
  <li>
\[index(root) = 0\]
  </li>
  <li>If \(l\) is the left child of \(n\), then \(index(l) = 2 \cdot index(n) + 1\)</li>
  <li>If \(r\) is the right child of \(n\), then \(index(r) = 2 \cdot index(n) + 2\)</li>
</ul>

<p>This can be very inefficient for unbalanced trees, for example, a tree which is just a “line” of nodes would grow with \(O(2^n)\) space, but it has a similarly good lookup time of \(O(log_2n)\)</p>
<h1 id="priority-queues">Priority queues</h1>
       
        <h2 id="priority-queues-adt">Priority queues (ADT)</h2>

<blockquote>
  <p><strong>Priority queues</strong> are (unsurprisingly) similar to queues, but items are sorted in order of a property “priority”, the assigned priorities specify which element leaves first (is dequeued). Unlike maps, multiple elements can have the same priority.</p>
</blockquote>

<p>These priorities, usually called keys, must form a total order relation, for example \(x \leq y\). We often use comparators on keys to form this total order relation.</p>

<table>
  <thead>
    <tr>
      <th>Fundamental Operations</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">enqueue(k,v)</code></td>
      <td>Insert an entry with key <strong><em>k</em></strong> and value <strong><em>v</em></strong> into the queue, where <strong><em>k</em></strong> determines its position in the queue.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">dequeue()</code></td>
      <td>Element with the highest priority is removed from the queue.</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">size()</code></td>
      <td>Size of priority queue</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">isEmpty()</code></td>
      <td>Returns <code class="language-plaintext highlighter-rouge">true</code> if priority queue is empty</td>
    </tr>
    <tr>
      <td><code class="language-plaintext highlighter-rouge">first()</code></td>
      <td>Returns the element with the highest priority – does not remove it.</td>
    </tr>
  </tbody>
</table>

<p><em>Note.</em> The names of these operations/methods can differ, it is important to understand their function and purpose to draw the link with concrete implementations.</p>

<h2 id="implementations">Implementations</h2>

<p>There are three common concrete implementations:</p>
<ul>
  <li>Unsorted list based</li>
  <li>Sorted list</li>
  <li>Heap based</li>
</ul>

<p>For both list based implementations, a positional/linked list should be used (for unsorted, doubly linked is needed), since we want to be able to grow the list, but don’t need to be able to index it</p>

<h3 id="unsorted-list-based">Unsorted list based</h3>

<p>To enqueue an item, we just add it to the end of the list, in \(O(1)\) time.</p>

<p>To dequeue an item, we have to traverse the entire list to find the smallest item, taking \(O(n)\) time</p>

<h3 id="sorted-list-based">Sorted list based</h3>

<p>To enqueue an item, we have to traverse the list to find where to put it, taking \(O(n)\) time (but we normally wouldn’t need to traverse the entire list, unlike dequeuing in the unsorted implementation, which also must)</p>

<p>To dequeue an item, we just take it from the front of the list, in \(O(1)\) time</p>

<h3 id="heap-based">Heap based</h3>

<p>This is covered in the section on heaps</p>

<h2 id="comparators">Comparators</h2>

<blockquote>
  <p><strong>Comparators</strong> are used to “encapsulate[…] the action of comparing two objects from a given total order”</p>

  <p><em>Data Structures and Algorithms in Java, Goodrich, Tamassia, Goldwasser</em>s</p>
</blockquote>

<p>The comparator is an object external to the keys being compared, not a property of the keys. See the 118 notes for a more full description.</p>

<p>In this context, comparators would be used to provide the total ordering on objects inserted to the priority queue.</p>

<h2 id="sorting-with-list-based-priority-queues">Sorting with list based priority queues</h2>

<p>We can sort a set of items by enqueueing them one by one, using the priority as the total ordering to sort by, and then dequeuing them into a list will result in them being sorted.</p>

<p>When the unsorted concrete implementation is used, this encodes “<strong>selection sort</strong>”. The steps taken in the sort are:</p>

<ol>
  <li>Enqueue all \(n\) elements, each taking \(O(1)\) time into the priority queue, taking \(O(n)\) time</li>
  <li>Dequeue all the elements into sorted order, with the total calls taking \(O(n) + O(n-1) + ... + O(1)\) which is \(O(n^2)\) time. Hence, the <strong>total time complexity</strong> is \(O(n^2)\)</li>
</ol>

<p>When the sorted concrete implementation is used, this encodes “<strong>insertion sort</strong>”. The steps taken in the sort are:</p>

<ol>
  <li>Enqueue \(n\) elements, with the total calls taking \(O(1) + O(2) + ... + O(n)\), which is \(O(n^2)\) time</li>
  <li>Dequeue all \(n\) items, each taking \(O(1)\), taking \(O(n)\) time. Hence, the <strong>total time complexity</strong> is \(O(n^2)\)</li>
</ol>

<h1 id="heaps">Heaps</h1>
       
        <h2 id="heaps-adt">Heaps (ADT)</h2>
<blockquote>
  <p><strong>Heaps</strong> are essentially binary trees storing keys at their nodes and satisfying a set of “heap properties”.</p>
</blockquote>

<p>As such, they are implemented in the same way as binary trees, discussed earlier, but with modified internal behaviour when inserting and deleting elements</p>

<h2 id="heap-properties">Heap properties</h2>

<p>The properties a binary tree must fulfil to be a heap are:</p>

<p><strong>Heap-order.</strong> For every internal node other than the root (as it has no parent), the value of the node is greater than the value of the parent node</p>

<p><strong>Complete binary tree.</strong> The height of the tree is minimal for the number of the nodes it contains, and is filled from “left to right”. This is formally defined as:</p>

<blockquote>
  <p>Let \(h\) be the height of the heap</p>

  <p>​	Every layer of height \(i\) other than the lowest layer (\(i = h-1\)) has \(2^i\) nodes</p>

  <p>​	In the lowest layer, the all internal nodes are to the left of external nodes</p>
</blockquote>

<p>The <strong>last node</strong> of the heap is the rightmost node of maximum depth</p>

<p><img src="./images/heapDiagram.png" alt="heapDiagram" class="center" /></p>

<p>Image source: <em>Data Structures and Algorithms in Java</em>, Goodrich, Tamassia, Goldwasser</p>

<h3 id="height-of-a-heap">Height of a Heap</h3>

<blockquote>
  <p>A heap storing <strong><em>n</em></strong> keys has height = log<sub>2</sub>n.</p>
</blockquote>

<p><strong>Proof.</strong> Let \(h\) be the height of a heap storing \(n\) keys</p>

<p>Since there are \(2^i\) keys at depth \(i = 0, \ldots, h - 1\) and at least 1 key at depth \(h\), we have \(n \ge 1 +2 +4+\ldots+2^{h-1} + 1\)</p>

<p>Thus, \(n \ge 2^h \Rightarrow h \le log_{2}\ n\).</p>

<h2 id="heap-methods">Heap methods</h2>

<h3 id="inserting-into-a-heap">Inserting into a heap</h3>

<p>First, the element is inserted to its temporary position of the rightmost node of maximum depth, so that it fills from left to right, with a running time of \(O(1)\) time, if a pointer to the position to insert is maintained</p>

<p><img src="./images/heapInsertOne.png" alt="heapInsertOne" class="center" /></p>

<p>Image source: <em>Data Structures and Algorithms in Java, Goodrich, Tamassia, Goldwasser</em></p>

<p>Then, the <code class="language-plaintext highlighter-rouge">upheap</code> algorithm is run to re-order the heap so that it fulfils the heap properties. This algorithm repeatedly swaps the inserted node with its parent, until either it reaches the root node, or it is larger than the parent node:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Let k &lt;- the element to insert
While k is smaller than its parent, and k is not the root node
	Swap the values of k and its parent node
</code></pre></div></div>

<p><img src="./images/heapInsertTwo.png" alt="heapInsertTwo" /></p>

<p>Image source: <em>Data Structures and Algorithms in Java, Goodrich, Tamassia, Goldwasser</em></p>

<p>Since the heap has a height of \(O(log_2\ n)\), performing a swap takes \(O(1)\) time, and the maximum number of swaps is the height of the heap, the upheap algorithm takes \(O(log_2\ n)\), time. In total, insertion takes \(O(log_2\ n)\) time.</p>

<h3 id="removal-from-a-heap">Removal from a heap</h3>

<p>The smallest item in the heap is the root node, so this value is stored and returned. However, we need to maintain heap properties as it is overwritten.</p>

<p>First, the value of the root node is overwritten with the value of the last node, and the last node is removed from the tree:</p>

<p><img src="./images/heapRemoveOne.png" class="center" /></p>

<p>Image source: <em>Data Structures and Algorithms in Java, Goodrich, Tamassia, Goldwasser</em></p>

<p>Then the <code class="language-plaintext highlighter-rouge">downheap</code> algorithm is run to re-order the heap so that it fulfils the heap properties:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Let p &lt;- the root node
Let c &lt;- the child of p with the minimal key (right if existent, otherwise left)
If the value of p is less than or equal to the value of c
	Stop, since the heap order property is fulfilled
Else
	Swap the values of p and c
	Run the downheap algorithm again with the root node (p) now as the child node (c)
</code></pre></div></div>

<p><img src="./images/heapRemoveTwo.png" alt="heapRemoveTwo" /></p>

<p>Image source: <em>Data Structures and Algorithms in Java, Goodrich, Tamassia, Goldwasser</em></p>

<p>As with <code class="language-plaintext highlighter-rouge">upheap</code>, since the heap has a height of \(O(log_2\ n)\), the <code class="language-plaintext highlighter-rouge">downheap</code> algorithm takes \(O(log_2\ n)\) time.</p>

<h2 id="use-in-sorting">Use in sorting</h2>

<p>Since the heap can be used to implement priority queues, it can be used for sorting as with list based implementations, which resulted in selection and insertion sort. This is called a heap sort.</p>

<p>The steps taken in heap sort are:</p>
<ol>
  <li>Enqueue \(n\) elements, with each enqueueing taking \(O(log n)\) time, so the total time is \(O(n \cdot log n)\) time</li>
  <li>Dequeue all \(n\) items, with each Dequeuing taking \(O(log n)\) time, so the total time is \(O(n \cdot log n)\) time</li>
</ol>

<p>Hence, the <strong>overall time complexity</strong> is \(O(n \cdot log n)\)</p>

<p>This is one of the fastest classes of sorting algorithm, and is much more efficient than <strong>quadratic sorting algorithms</strong> like insertion or selection sort.</p>

<h2 id="concrete-implementations">Concrete implementations</h2>

<p>Any <strong>tree implementation</strong> can be used for a heap, as it merely modifies the way getters and setters work, not the internal data structures.</p>

<p>The main draw-back of <strong>array based implementations</strong> of space inefficiency for unbalanced trees is a <strong>non-issue for heaps</strong>, as they are implicitly balanced, so they are often used.</p>
<h1 id="skip-lists">Skip Lists</h1>
       
        <h2 id="motivations-for-skip-lists">Motivations for skip lists</h2>

<p>We want to be able to efficiently implement both searching, and insertion and deletion</p>

<p>For fast searching, we need the list to be sorted, and we have come across two concrete implementations of lists, but neither of which fulfil both of  these goals.</p>

<ul>
  <li>Sorted arrays
    <ul>
      <li>Easy to search using binary search, since they are not indexable, needs \(O(log\ n)\) time</li>
      <li>Difficult insert/delete from, as elements need to be “shuffled up” to maintain ordering, needs \(O(n)\) time</li>
    </ul>
  </li>
  <li>Sorted lists
    <ul>
      <li>Easy to insert/delete from, assuming the position is known, needs \(O(1)\) time</li>
      <li>Difficult to search, since they are not indexable, needs \(O(n)\) time</li>
    </ul>
  </li>
</ul>

<h2 id="skip-lists-adt">Skip Lists (ADT)</h2>

<p><strong>Skip lists</strong> are composed from a number of sub-lists, which act as layers within them, which we denote by the set \(S = \{S_0, S_1, ..., S_h\}\) where \(h\) denotes the number of layers in the list, i.e. its “height”</p>

<ul>
  <li>
    <p>All lists have a guard values \(+ \infty\) and \(- \infty\) at either end, and all the elements are in order between those values</p>
  </li>
  <li>
    <p>The “bottom” list, \(S_0\) contains all the values in order between the guards</p>
  </li>
  <li>
    <p>The “top” list, \(S_h\), contains only the guard values, \(+ \infty\) and \(- \infty\)</p>
  </li>
  <li>
    <p>Each list \(S_i\) for \(0 &lt; i &lt; h\) (i.e. everything bar the top list, which contains only the guards, and the bottom list, which contains all elements) contains a random subset of the elements in the list below it, \(S_1\)</p>
  </li>
  <li>
    <p>The probability of an element in \(S_i\) being in the list above it, \(S_{i+1}\), is \(0.5\)</p>
  </li>
</ul>

<p>A diagram of the structure of a skip list is shown below</p>

<p><img src="./images/skipLists.png" alt="skipLists" /></p>

<p>Image source: <em>Data Structures and Algorithms in Java, Goodrich, Tamassia, Goldwasser</em></p>

<h2 id="searching">Searching</h2>

<p>To search for an value <code class="language-plaintext highlighter-rouge">v​</code> in a skip list, we follow the algorithm</p>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">Algorithm</span> <span class="nf">search</span><span class="o">(</span><span class="n">k</span><span class="o">):</span>
  <span class="n">p</span> <span class="o">&lt;-</span> <span class="n">skiplist</span><span class="o">.</span><span class="na">first</span><span class="o">()</span> <span class="c1">// this is the minus-infinity guard of the top list</span>
  <span class="nc">Repeat</span>
    <span class="n">e</span> <span class="o">&lt;-</span> <span class="n">p</span><span class="o">.</span><span class="na">next</span><span class="o">().</span><span class="na">element</span><span class="o">()</span>
    <span class="k">if</span> <span class="n">e</span><span class="o">.</span><span class="na">key</span><span class="o">()</span> <span class="o">==</span> <span class="n">k</span>
      <span class="k">return</span> <span class="n">e</span>
    <span class="k">else</span> <span class="k">if</span> <span class="n">e</span><span class="o">.</span><span class="na">key</span><span class="o">()</span> <span class="o">&gt;</span> <span class="n">k</span> <span class="c1">// next element is greater than v</span>
      <span class="n">p</span> <span class="o">&lt;-</span> <span class="n">p</span><span class="o">.</span><span class="na">below</span><span class="o">()</span>    <span class="c1">// Drop Down Step</span>
      <span class="k">if</span> <span class="n">p</span> <span class="o">==</span> <span class="kc">null</span> <span class="n">then</span> <span class="k">return</span> <span class="kc">null</span>
    <span class="k">else</span>                <span class="c1">// next element's key is smaller than v</span>
      <span class="n">p</span> <span class="o">&lt;-</span> <span class="n">p</span><span class="o">.</span><span class="na">next</span><span class="o">()</span>     <span class="c1">// Scan Forward Step</span>
</code></pre></div></div>

<p><img src="./images/skipListsSearch.png" alt="skipListsSearch" /></p>

<p>Image source: <em>Data Structures and Algorithms in Java, Goodrich, Tamassia, Goldwasser</em></p>

<h2 id="inserting">Inserting</h2>

<p>To insert a value <code class="language-plaintext highlighter-rouge">v</code>​ into a skip list, we follow the algorithm.</p>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">i</span> <span class="o">&lt;-</span> <span class="n">number</span> <span class="n">of</span> <span class="n">flips</span> <span class="n">of</span> <span class="n">a</span> <span class="n">fair</span> <span class="n">coin</span> <span class="n">before</span> <span class="n">a</span> <span class="n">head</span> <span class="n">comes</span> <span class="n">up</span>
<span class="nc">If</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="n">height</span> <span class="n">of</span> <span class="n">skip</span> <span class="n">list</span>
  <span class="nc">Add</span> <span class="k">new</span><span class="o">,</span> <span class="n">empty</span><span class="o">,</span> <span class="n">sub</span><span class="o">-</span><span class="n">lists</span> <span class="o">{</span><span class="no">S</span><span class="o">(</span><span class="n">h</span><span class="o">+</span><span class="mi">1</span><span class="o">),</span> <span class="o">...,</span> <span class="no">S</span><span class="o">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="o">)}</span> <span class="n">to</span> <span class="no">S</span> 
<span class="nc">Using</span> <span class="n">the</span> <span class="n">search</span> <span class="n">algorithm</span><span class="o">,</span> <span class="n">we</span> <span class="n">find</span> <span class="n">v</span> <span class="c1">//even though we know it is not inserted</span>
  <span class="nc">For</span> <span class="n">every</span> <span class="n">dropdown</span> <span class="n">step</span><span class="o">,</span> <span class="n">store</span> <span class="n">the</span> <span class="n">position</span> <span class="n">of</span> <span class="n">the</span> <span class="n">element</span> <span class="n">in</span> <span class="n">an</span> <span class="n">array</span>
	<span class="c1">// This array stores the positions p(0) to p(i) of the </span>
  <span class="c1">// largest element lesser than v of each sublist S(j)</span>
<span class="nc">For</span> <span class="n">each</span> <span class="n">sublist</span> <span class="n">from</span> <span class="mi">0</span> <span class="n">to</span> <span class="n">i</span>
	<span class="nc">Insert</span> <span class="n">v</span> <span class="n">into</span> <span class="nf">S</span><span class="o">(</span><span class="n">j</span><span class="o">)</span> <span class="n">immediately</span> <span class="n">after</span> <span class="n">the</span> <span class="n">position</span> <span class="nf">p</span><span class="o">(</span><span class="n">j</span><span class="o">)</span> <span class="n">in</span> <span class="n">array</span>
</code></pre></div></div>

<p><img src="./images/skipListsInsertion.png" alt="skipListsInsertion" /></p>

<p>Image source: <em>Data Structures and Algorithms in Java, Goodrich, Tamassia, Goldwasser</em></p>

<h2 id="deleting">Deleting</h2>

<p>To delete a value <code class="language-plaintext highlighter-rouge">v</code>​ from a skip list, we follow the algorithm</p>

<div class="language-java highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">Using</span> <span class="n">search</span> <span class="n">algorithm</span><span class="o">,</span> <span class="n">find</span> <span class="n">v</span> <span class="n">in</span> <span class="n">skiplist</span>
  <span class="nc">Once</span> <span class="n">found</span> <span class="n">at</span> <span class="n">position</span> <span class="n">p</span><span class="o">,</span>
    <span class="k">while</span> <span class="n">p</span><span class="o">.</span><span class="na">below</span><span class="o">()</span> <span class="o">!=</span> <span class="kc">null</span>
      <span class="n">hold</span> <span class="o">&lt;-</span> <span class="n">p</span>
      <span class="nf">delete</span><span class="o">(</span><span class="n">p</span><span class="o">)</span> <span class="c1">// Delete v from sublists below</span>
      <span class="n">p</span> <span class="o">&lt;-</span> <span class="n">hold</span>
<span class="nc">Remove</span> <span class="n">all</span> <span class="n">but</span> <span class="n">one</span> <span class="n">list</span> <span class="n">containing</span> <span class="n">only</span> <span class="n">guards</span> <span class="n">from</span> <span class="n">the</span> <span class="n">top</span> <span class="n">of</span> <span class="n">the</span> <span class="n">skip</span> <span class="n">list</span>
</code></pre></div></div>

<p><img src="./images/skipListsDeletion.png" alt="skipListsDeletion" /></p>

<p>Image source: <em>Data Structures and Algorithms in Java, Goodrich, Tamassia, Goldwasser</em></p>

<h2 id="implementation">Implementation</h2>

<p>We can use “quad-nodes”, which are similar to those used in linked lists, but with four pointers, instead of just one to store the entry, and links to the previous, next, below and above nodes:</p>

<p><img src="./images/skipListsQuadNode.png" alt="skipListsQuadNode" class="center" /></p>

<p>Image source: <em>Data Structures and Algorithms in Java, Goodrich, Tamassia, Goldwasser</em></p>

<p>Additionally, there are special guard nodes, with the values \(+ \infty\) and \(- \infty\), and fewer pointers, as they don’t have adjacencies on one side.</p>

<h2 id="performance">Performance</h2>

<h3 id="space-usage">Space usage</h3>

<p>Dependent on randomly generated numbers for how many elements are in high layers, and how high the layers are.</p>

<p>We can find the <strong>expected number of nodes</strong> for a skip list of \(n\) elements:</p>

<blockquote>
  <p>The probability of having \(i\) layers in the skip list is \(\frac{1}{2^i}\).</p>

  <p>If the probability of any one of \(n\) entries being in a set is \(p\), the <strong>expected size</strong> of the set is \(n \cdot p\)</p>

  <p>Hence, the expected size of a list \(S_i\) is \(\frac{n}{2^i}\)</p>

  <p>This gives the expected number of elements in the list as \(\sum_{i=0}^{h}(\frac{n}{2^i}),\) where \(h\) is the height.</p>

  <p>We can express this as \(n \cdot \sum_{i=0}^{h}(\frac{1}{2^i}) \lt 2n\), and with the sum <strong>converging</strong> to a <strong>constant factor</strong>, so the <strong>space complexity</strong> is \(O(n)\).</p>
</blockquote>

<h3 id="height">Height</h3>

<p>The height of a skip list of \(n\) items is <strong>likely</strong> to (since it is generated randomly) have a height of order \(O(log\ n)\).</p>

<p>We show this by taking a height logarithmically related to the number of elements, and showing that the probability of the skip list having a height greater than that is very small.</p>

<blockquote>
  <p>The probability that a layer \(S_i\) has at least one item is at most \(\frac{n}{2^i}\)</p>

  <p>Considering a layer logarithmically related to the number of elements \(i = 3 \cdot log\ n\)</p>

  <p>The probability of the layer \(S_i\) has at least one entry is at most \(\frac{n}{2^{3 \cdot log\ n}} = \frac{n}{n^3} = \frac{1}{n^2}\)</p>

  <p>Hence, the probability of a skip list of \(n\) items having a height of more than \(3 \cdot log\ n\) is at most \(\frac{1}{n^2}\), which tends to a negligibly small number very quickly.</p>
</blockquote>

<h3 id="search-time">Search time</h3>

<p>The search time is <strong>proportional</strong> to the number of steps scan forward and drop down steps.</p>

<blockquote>
  <p>In the worst case, both dimensions have to be totally traversed, if the item is both bigger than all other items, or not present.</p>

  <p>The number of <strong>drop down steps</strong> is <strong>bounded</strong> by the height (\(\approx O(log\ n)\) with high probability), therefore it is <strong>trivial</strong> to analyse it.</p>

  <p>To analyse the scan-forward step, firstly <a href="#inserting">recall</a> that the number of sub-lists an item appears in is the number of times our flipped fair coin gives us a <strong>success condition</strong> (as above we use <strong>heads</strong> as the success condition and tails as failure – you can choose either).</p>

  <p>A probabilistic fact is that the <strong>expected</strong> number of coin tosses to get heads is 2. <a href="#coin-toss-expectation-explanation">Why?</a></p>

  <p>When we scan forward, the element we scan forward to does not belong to a higher sub-list. Therefore a scan forward step is associated with a <strong>success condition</strong>, i.e coin giving us heads. So, the <strong>expected number of scan-forward</strong> steps for <strong>each sub-list</strong> is 2.</p>

  <p>Hence, the expected number of scan forward steps in <strong>total</strong> is \(O(log\ n)\)</p>
</blockquote>

<p>Hence, the total <strong>search time</strong> is \(O(log\ n)\).</p>

<h3 id="update-time">Update time</h3>

<p>Since the insert and delete operations are both essentially wrappers around the search operation, and all of their additional functionality is of \(O(log\ n)\) or better, the time complexity is the same as the search function</p>

<h3 id="coin-toss-expectation-explanation">Coin Toss Expectation Explanation</h3>

<p><em>FYI ONLY.</em> The source of this explanation is by <a href="https://math.stackexchange.com/questions/1196452/expected-value-of-the-number-of-flips-until-the-first-head">JMoravitz on Stack Exchange</a> (Accessed 16 May 2021)</p>

<blockquote>
  <p>Let X be a <strong>discrete random variable</strong> with possible outcomes:</p>

  <p>\(x1,x2,x3,…,xi,…\) with associated probabilities \(p1,p2,p3,…,pi,…\)</p>

  <p>The <strong>expected value</strong> of \(f(X)\) is given as: \(E[f(X)] = \sum\limits_{i\in\Delta} f(x_i)p_i\)</p>
</blockquote>

<p>For a coin toss, \(X\) could be \(1,2,3,\ldots,i,\ldots\) with corresponding probabilities \(\frac{1}{2},\frac{1}{4},\frac{1}{8},\dots,\frac{1}{2^i},\dots\)</p>

<p>So, the expected value of \(X\) is: \(\sum\limits_{i=1}^\infty i(\frac{1}{2})^i=\frac{1}{0.5}=2\)</p>

<p>This is a well known infinite sum of the form \(\sum\limits_{i=1}^\infty i p (1-p)^{i-1}=\frac1p\)</p>

<p>To prove this:</p>

<p>[\sum\limits_{i=1}^\infty i p (1-p)^{i-1} = p\sum\limits_{i=1}^\infty i (1-p)^{i-1}<br />
= p\left(\sum\limits_{i=1}^\infty (1-p)^{i-1} + \sum\limits_{i=2}^\infty (1-p)^{i-1} + \sum\limits_{i=3}^\infty (1-p)^{i-1} + \dots\right)<br />
= p\left[(1/p)+(1-p)/p+(1-p)^2/p+\dots\right]<br />
= 1 + (1-p)+(1-p)^2+\dots<br />
=\frac{1}{p}]</p>
<h1 id="binary-search-and-self-balancing-trees">Binary search and self-balancing trees</h1>
       
        <h1 id="binary-search-trees">Binary search trees</h1>

<p><strong>Binary search trees</strong> can be used as a concrete implementation of ordered maps, with items being stored in the tree ordered by their key</p>

<ul>
  <li>Search tables are another concrete implementation of ordered maps, but instead use a sorted sequence, normally an array, which is searchable with binary search in \(O(log\ n)\), but requires \(O(n)\) for insertion and removal. This means they are only effective for either small maps, or cases where there are few insertions and deletions</li>
</ul>

<p>They support nearest neighbour queries, finding next highest and next lowest items</p>

<h2 id="properties">Properties</h2>

<p>The properties of binary search trees are:</p>

<ul>
  <li>External nodes store no items</li>
  <li>All left children of any internal node have a smaller key than their parent node</li>
  <li>All right children of any internal node have a larger key than their parent node</li>
  <li>In-order traversals yield a sequence of the keys in ascending order</li>
</ul>

<h2 id="operations">Operations</h2>

<h3 id="searching">Searching</h3>

<p>Start at the root, and recursively proceed down the appropriate subtrees until the key or an external node is found</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Function Search(n, k)
	If n is an external node
		Return that the key is not in the tree
	Else if the key being searched for is less than the value of the current node
		Return Search called on the left child of n, and the same k
	Else if the key being searched for is greater than the value of the current node
		Return Search called on the right child of n, and the same k
	Else (the key is equal to the one being search for)
		Return that the key is in the tree

Let r &lt;- the root node of the tree to search
Let k &lt;- the key to search for
Search(r, k)
</code></pre></div></div>

<h3 id="insertion">Insertion</h3>

<p>Perform the searching operation, but when an external node is found, instead of returning that the key is not present, set that internal node as the key to insert, and give it two external child nodes</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Let e &lt;- the external node terminating the search algorithm
Set the value of e to the value of the node to insert
Add two extenral child nodes to e so that it is now internal
</code></pre></div></div>

<h3 id="deletion">Deletion</h3>

<p>Dependent on the number of children of the node to delete, different operations are needed to delete the node</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>If the node has no internal children
	Overwrite the node to become an empty external node
Else if the node has only one internal child
	Overwrite it with the internal child node
Else (the node has two internal children)
	Let i &lt;- node that immediately follows it in an in-order traversal
	Overwrite the node with a copy of i
	Set i to be external, and discard its left child, which must itself be an external node
</code></pre></div></div>

<h2 id="algorithm">Algorithm</h2>

<p>In all cases, the space complexity is \(O(n)\)</p>

<p>The time complexity for searching, inserting and deleting is dependent on the height of the tree:</p>
<ul>
  <li>If the tree is balanced, then the height is \(log\ n\), so the time for these operations is \(O(log\ n)\)</li>
  <li>In the worst case, the tree can be totally unbalanced, just a straight line of internal nodes, in which case the height is \(n\), so the time for these operations is \(O(n)\)</li>
</ul>

<h1 id="avl-trees">AVL trees</h1>

<p><strong>AVL trees</strong> are a concrete implementation of self-balancing binary search tree, with insertion and deletion operations designed to re-arrange the tree to ensure it remains balanced. It is named after its creators, Adelson-Velsky and Landis</p>

<p>Other self-balancing binary search trees exist, such as red-black trees, but this is a common approach to implementing such an ADT</p>

<h2 id="properties-of-a-balanced-tree">Properties of a balanced tree</h2>

<p>The a tree to be balanced:</p>

<ul>
  <li>Each node has at most two child nodes</li>
  <li>For every internal node in the tree, the heights of the child subtrees can differ by at most one</li>
</ul>

<p>These requirements ensure that the maximum height to store \(n\) keys is \(log\ n\), which can be proved by induction</p>

<h2 id="operations-1">Operations</h2>

<p>Searching is approached as it is in a normal binary search tree</p>

<h3 id="tri-node-restructuring">Tri-node restructuring</h3>

<h3 id="re-balancing-vs-restructuring">Re-balancing VS restructuring</h3>

<h3 id="insertion-1">Insertion</h3>

<h3 id="deletion-1">Deletion</h3>

<h2 id="performance">Performance</h2>

<p>In all cases, the space complexity is \(O(n)\), and searching takes \(O(log\ n)\) time -  as with any balanced binary tree</p>

<p>Insertion and deletion are also \(O(log\ n)\), as searching for the element is \(O(log\ n)\), and then restructuring the tree to maintain the balance property is \(O(log\ n)\), so the total is also \(O(log\ n)\)</p>
<h1 id="graphs">Graphs</h1>
       
        <h1 id="graphs">Graphs</h1>

<h2 id="graphs-as-a-mathematical-concept">Graphs as a mathematical concept</h2>

<p><strong>Graphs</strong> are defined as a pair \(G = (V, E)\) were \(V\) is a set of vertices, and \(E\) is an unordered collection of pairs of vertices, called edges, for example: \(G = (\{a, b, c\}, [(a,b), (b,c), (c,a)])\)</p>

<p>Directed and undirected graphs</p>
<ul>
  <li>
    <p>In undirected graphs, the edge pair indicates that both vertices are connected to each other</p>
  </li>
  <li>
    <p>In directed graphs, the edge pair indicates that the first vertex is connected to the second, but not vice versa</p>
  </li>
</ul>

<h3 id="terminology">Terminology</h3>

<ul>
  <li>Adjacent vertices - vertices with an edge between them</li>
  <li>Edges incident on a vertex - edges which both connect to the same vertex</li>
  <li>End vertices/endpoints - the two vertices in the pair that an edge connects to</li>
  <li>Degree of a vertex - the number of edges that connect to a pair</li>
  <li>Parallel edges - two edges both connecting the same nodes (reason why edges are an unordered collection, not a set)</li>
  <li>Self-loop - an edge whose vertices are both the same</li>
  <li>Path - a sequence of alternating vertices and edges, starting and ending in a vertex</li>
  <li>Simple paths - paths containing no repeating vertices (hence are acyclic)</li>
  <li>Cycle - a path starting and ending at the same vertex</li>
  <li>Acyclic - a graph containing no cycles</li>
  <li>Simple cycle - a path where the only repeated vertex is the starting/ending one</li>
  <li>Length (of a path of cycle) - the number of edges in the path/cycle</li>
  <li>Tree - a connected acyclic graph</li>
</ul>

<h3 id="graph-properties">Graph properties</h3>

<ol>
  <li>The sum of the degrees of the vertices in an undirected graph is an even number
    <ul>
      <li>Handshaking theorem - every edge must connect two vertices, so sum of degrees is twice the number of edges, which must be even</li>
    </ul>
  </li>
  <li>
    <p>An undirected graph with no self loops nor parallel edges, with number of edges \(m\) and number of vertices \(n\) fulfils the property \(m \leq \frac{n \cdot (n-1)}{2}\)</p>

    <ul>
      <li>
        <p>The first vertex can connect to \(n-1\) vertices (all vertices bar itself), then the second can connect to \(n-2\) (all the vertices bar itself and the first vertex, which it is already connected to), and so on, giving the sum \(1+2+...+n\) , which is known to be \(\frac{n \cdot (n-1)}{2}\)</p>
      </li>
      <li>
        <p>Fully connected graphs fulfil the property \(m = \frac{n \cdot (n-1)}{2}\)</p>
      </li>
    </ul>
  </li>
</ol>

<h2 id="graphs-as-an-adt">Graphs as an ADT</h2>

<p><strong>Graphs</strong> are a “collection of vertex and edge objects”</p>

<p>They have a large number of fundamental operations, to the extent it is unnecessary to enumerate them here, but they are essentially just accessor and mutator and count methods on the vertices and edges</p>

<p>There are three main concrete implementations of the graph ADT</p>

<ul>
  <li>
    <p>Edge list</p>

    <ul>
      <li>One list of vertices</li>
      <li>One list of edges, each of which contain references to their endpoint vertices</li>
    </ul>

    <p><img src="images\edgeListGraph.png" alt="edgeListGraph" /></p>

    <p>Image source: <em>Data Structures and Algorithms in Java, Goodrich, Tamassia, Goldwasser</em></p>
  </li>
  <li>
    <p>Adjacency list</p>

    <ul>
      <li>Array containing a all of the nodes, each of which have a pointer to a list of the other nodes they connect to</li>
    </ul>

    <p><img src="images\adjacencyListGraph.png" alt="adjacencyListGraph" /></p>

    <p>Image source: <em>Data Structures and Algorithms in Java, Goodrich, Tamassia, Goldwasser</em></p>
  </li>
  <li>
    <p>Adjacency matrix</p>

    <ul>
      <li>2D array acts a lookup table for whether vertices have an edge connecting them</li>
      <li>Square matrix, with each dimension being the number of vertices in the graph</li>
      <li>Undirected graphs are symmetrical along the leading diagonal</li>
    </ul>

    <p><img src="images\adjacencyMatrixGraph.png" alt="adjacencyMatrixGraph" /></p>

    <p>Image source: <em>Data Structures and Algorithms in Java, Goodrich, Tamassia, Goldwasser</em></p>
  </li>
</ul>

<h2 id="more-terminology">More terminology</h2>

<h3 id="subgraphs">Subgraphs</h3>

<p>A subgraph of the graph \(G\) fulfils the two properties:</p>

<ul>
  <li>
    <p>Its vertices are a subset of the vertices of \(G\)</p>
  </li>
  <li>
    <p>Its edges are a subset of the edges of \(G\)</p>
  </li>
</ul>

<p>A spanning subgraph contains all of the vertices in \(G\). This then gives rise to spanning trees, which are spanning subgraphs which are connected and acyclic</p>

<h1 id="depth-first-search">Depth-first search</h1>

<p>Depth-first search is a technique to traverse graphs. It takes \(O(n + m)\) time to search a graph of \(n\) vertices and \(m\) edges.</p>

<p>Informally, it can be described as always proceeding to its first adjacency, then backtracking when it reaches a vertex with no adjacencies which it has not explored already</p>

<blockquote>
  <p><strong>Algorithm</strong> \(DFS(G, v)\)
		<strong>Input</strong>  graph \(G\) and start at vertex \(v\) of \(G\)
		<strong>Output</strong> labeling of the edges of \(G\) in the connected component of v as discovery edges and back edges
		\(setLabel(v, VISITED)\)
		<strong>for all</strong> \(e \in G.incidentEdges(v)\)
		    <strong>if</strong> \(getLabel(e) = UNEXPLORED\)
		 		   \(w \leftarrow opposite(v,e)\)
		 		   <strong>if</strong> \(getLabel(w) = UNEXPLORED\)
		 		 		  \(setLabel(e, DISCOVERY)\)
		 		 		  \(DFS(G, w)\)
		 		   <strong>else</strong>
		 		 		  \(setLabel(e,BACK)\)
<strong>END ALGORITHM</strong></p>
</blockquote>

<p><img src="\images\dfsExample.png" alt="dfsExample" /></p>

<p>Image source: <em>Data Structures and Algorithms in Java, Goodrich, Tamassia, Goldwasser</em></p>

<p>It has the following properties</p>

<ul>
  <li>It visits all vertices and edges in any connected component of a graph</li>
  <li>The discovery edges form a spanning tree of any graph it traverses</li>
</ul>

<p>It can be used for path-finding by performing the traversal until the target node is found, then backtracking along the discovery edges to find the reverse of the path</p>

<p>It can be used to identify cycles, as if it ever finds an adjacency to a vertex which it has already explored, (a back edge), the graph must contain a cycle</p>

<h1 id="breadth-first-search">Breadth-first search</h1>

<p>Breadth-first search is a technique to traverse graphs. It takes \(O(n + m)\) time to search a graph of \(n\) vertices and \(m\) edges.</p>

<p>Informally, it can be described as exploring every one of its adjacencies, then proceeding to the first adjacency, then backtracking when it reaches a vertex with no adjacencies which it has not explored already</p>

<blockquote>
  <p><strong>Algorithm</strong> \(BFS(G)\)
		<strong>Input</strong> graph \(G\)
		<strong>Output</strong> labeling of the edges and partition of the vertices of G
		<strong>for all</strong> \(e \in G.vertices()\)
		    \(setLabel(u, UNEXPLORED)\)
		<strong>for all</strong> \(e \in G.edges()\)
		    \(setLabel(e, UNEXPLORED)\)
		<strong>for all</strong> \(v \in G.vertices()\)
		    <strong>if</strong> \(getLabel(v) = UNEXPLORED\)
		 		   \(BFS(G,v)\)
<strong>END ALGORITHM</strong></p>
</blockquote>

<p><img src="\images\bfsExample.png" alt="bfsExample" /></p>

<p>Image source: <em>Data Structures and Algorithms in Java, Goodrich, Tamassia, Goldwasser</em></p>

<p>It has the following properties</p>

<ul>
  <li>It visits all vertices and edges in any connected component of a graph</li>
  <li>The discovery edges form a spanning tree of any graph it traverses</li>
  <li>The path between any two vertices in the spanning tree of discovery edges it creates is the shortest path between them in the graph</li>
</ul>

<p>It can be used for path-finding by performing the traversal until the target node is found, then backtracking along the discovery edges to find the reverse of the path</p>

<p>It can be used to identify cycles, as if it ever finds an adjacency to a vertex which it has already explored, (a back edge), the graph must contain a cycle</p>

<h1 id="directed-graphs">Directed graphs</h1>

<p>Directed graphs (digraphs) are graphs where every edge is directed. It can be applied to dependency and scheduling problems. When representing it in concrete implementations, we tend to keep in and out edges separately</p>

<p>It has the following properties:</p>

<ul>
  <li>If a simple directed graph has \(m\) edges and $n$ vertices, then \(m \leq n \cdot (n-1)\), since every vertex can connect to every other vertex bar itself</li>
</ul>

<p>There is more terminology specifically about digraphs:</p>

<ul>
  <li>
    <p>One vertex is said to be reachable from the other if there exists a directed path from the other to it</p>
  </li>
  <li>
    <p>A digraph is said to be strongly connected if each vertex is reachable from every other vertex</p>
    <ul>
      <li>We can identify strong connectivity by running DFS or BFS, and seeing if the vertex is ever reached in the traversal. This has a running time of \(O(n+m)\)</li>
      <li>It is also possible to create maximal subgraphs with every vertex being reachable in \(O(n+m)\) time, but this is more involved</li>
    </ul>
  </li>
</ul>

<h2 id="transitive-closure">Transitive closure</h2>

<p>The transitive closure provides reachability information about a digraph</p>

<p>Given a digraph \(G\), the transitive closure of \(G\) is the digraph \(G*\) such that</p>

<ul>
  <li>\(G*\) has the same vertices as \(G\)</li>
  <li>If \(G\) has a directed path from \(u\) to \(v\), and \(u \neq v\), then \(G*\) has a directed edge from \(u\) to \(v\)</li>
</ul>

<p>Informally, this means that every pair of vertices with a path between them is adjacent</p>

<p><img src="\images\transitiveClosure.png" alt="transitiveClosure" /></p>

<p>Image source: <em>Data Structures and Algorithms in Java, Goodrich, Tamassia, Goldwasser</em></p>

<p>We can naively compute this by performing DFS for each vertex in graph to identify every reachable edge from it, then setting edges between them. However, this is very slow, being \(O(n \cdot (n+m))\) time.</p>

<p>Instead, we can use the Floyd-Warshall algorithm, which is a dynamic programming solution:</p>

<p><img src="\images\floydWarshall.png" alt="floydWarshall" /></p>

<p>Image source: <em>Data Structures and Algorithms in Java, Goodrich, Tamassia, Goldwasser</em></p>

<p>We build up from \(1\) to \(k\), starting with the base case of the initial graph, which only has the initial adjacencies. We then add edges between any included nodes with path length two between them.</p>

<p>With each iteration, we introduce a new node considered in the temporary graph, and ensure that all edges within this temporary graph are transitively closed.</p>

<p>Since at the end of every step, every node is transitively closed, when all nodes are included, the entire graph is transitively closed</p>

<blockquote>
  <p><strong>Algorithm</strong> \(FloydWarshall(G)\)
		<strong>Input</strong> digraph \(G\)
		<strong>Output</strong> transitive closure \(G^*\) of \(G\)
		\(i \leftarrow 1\)
		<strong>for all</strong> \(v \in G.vertices()\)
		    denote \(v\) as \(v_i\)
		    \(i \leftarrow i + 1\)
		\(G_0 \leftarrow G\)
		<strong>for</strong> \(k \leftarrow 1\) <strong>to</strong> \(n\) <strong>do</strong>
		    \(G_k \leftarrow G_{k-1}\)
		 		   <strong>for</strong> \(i\leftarrow 1\) <strong>to</strong> \(n(i\neq k)\) <strong>do</strong>
		 		 		  <strong>for</strong> \(j \leftarrow 1\) <strong>to</strong> \(n(j\neq i, k)\) <strong>do</strong>
		 		 		 		 <strong>if</strong> \(G_{k-1}.areAdjacent(v_i,v_k)\)  \(\&amp;\) \(G_{k-1}.areAdjacent(v_k,v_j)\)
		 		 		 		 		<strong>if</strong> \(¬G_{k-1}.areAdjacent(v_i,v_j)\)
		 		 		 		 		    \(G_k.insertDirectedEdge(v_i,v_j,k)\)
		    <strong>return</strong> \(G_n\)
<strong>END ALGORITHM</strong></p>
</blockquote>

<p>Running time is \(O(n^3)\), which is better than \(O(n \cdot (n+m))\) for non-sparse graphs (often graphs have many more edges than nodes)</p>

<h2 id="topological-ordering">Topological ordering</h2>

<p>A graph has a topological ordering if it is directed and acyclic</p>

<ul>
  <li>Having cycles would informally be self-dependencies</li>
  <li>This can be done in \(O(m+n)\) time using DFS</li>
</ul>

<p>Topological ordering means the same thing as a total relation in CS130, if the graph is considered as a set of relations</p>
<h1 id="general-algorithms">General algorithms</h1>
       
        <h1 id="searching-data-structures">Searching data structures</h1>

<h2 id="linear-search">Linear search</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Let arr &lt;- the array to search
Let k &lt;- the item to search for
Let n &lt;- 0

While n is smaller than the length of arr
	If k is equal to arr[n]
		Stop, since the item is found
	Increment n
Stop, since the item is not in the array
</code></pre></div></div>

<h2 id="binary-search">Binary search</h2>

<p>Must be performed on a sorted list</p>

<h3 id="iterative-algorithm">Iterative algorithm</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Let arr &lt;- the array to search
Let k &lt;- the item to search for
Let l &lt;- 0
Let r &lt;- the size of arr - 1
Let m &lt;- (l+r) / 2

While l != r
	If k is equal to arr[n]
		Stop, since the item is found
	Else if k is less than arr[n]
		r &lt;- m - 1
		m &lt;- (l+r) / 2
	Else (if k is greater than arr[n])
		l &lt;- m
		m &lt;- (l+r) / 2
Stop, since the item is not in the array
</code></pre></div></div>

<h3 id="recursive-algorithm">Recursive algorithm</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Let arr &lt;- the array to search
Let k &lt;- the item to search for

Function binarySearch(arr, k)
	Let l &lt;- 0
    Let r &lt;- the size of arr - 1
    Let m &lt;- (l+r) / 2
    If l == m
    	Stop, since the item is not in the array
    Else if k is equal to arr[n]
		Stop, since the item is found
	Else if k is less than arr[n]
		binarySearch(arr[l:m], k)
	Else (if k is greater than arr[n])
		binarySearch(arr[m:r], k)
</code></pre></div></div>

<h1 id="sorting-data-structures">Sorting data structures</h1>

<h2 id="insertion-sort">Insertion sort</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Let P &lt;- a priority queue using an sorted array implementation
Let arr &lt;- the array to sort
Let arr' &lt;- the sorted array
For each i in arr
	Enqueue i to P
While P is not empty
	Let i &lt;- Dequeue from P
	Append i to arr'
</code></pre></div></div>

<h2 id="selection-sort">Selection sort</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Let P &lt;- a priority queue using an unsorted array implementation
Let arr &lt;- the array to sort
Let arr' &lt;- the sorted array
For each i in arr
	Enqueue i to P
While P is not empty
	Let i &lt;- Dequeue from P
	Append i to arr'
</code></pre></div></div>

<h2 id="heap-sort">Heap sort</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Let P &lt;- a priority queue using a heap based implementation
Let arr &lt;- the array to sort
Let arr' &lt;- the sorted array
For each i in arr
	Enqueue i to P
While P is not empty
	Let i &lt;- Dequeue from P
	Append i to arr'
</code></pre></div></div>

<h2 id="merge-sort">Merge sort</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Let arr &lt;- the array to sort

Function mergeSort(arr)
	If arr contains only one element
		Return arr
    Let lArr, rArr &lt;- arr split into two even halves
    Return merge(
    	mergeSort(lArr),
    	mergeSort(rArr)
    )
    
Function merge(arr1, arr2)
	Let arr' &lt;- an empty array large enough to fit both arr1 and arr2 in
	Let n1, n2 &lt;- 0
	While neither arr1 nor arr2 are empty
		If arr1[n1] = arr2[n2]
			Append arr1[n1] and arr2[n2] to arr'
			Increment n1 and n2
		Else if arr1[n1] &lt; arr2[n2]
			Append arr1[n1] to arr'
			Increment n1
		Else (if arr1[n1] &gt; arr2[n2])
			Append arr2[n2] to arr'
			Increment n2
	For each element in arr1 from n1 to its last element
		Append arr1[n1] to arr'
	For each element in arr2 from n2 to its last element
		Append arr2[n2] to arr'
	Return arr'
</code></pre></div></div>

<h1 id="reversing-data-structures">Reversing data structures</h1>
<h2 id="reversing-a-stack">Reversing a stack</h2>

<p>Push all the items in array to the stack, then pop all the items off the stack into the new reversed array</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Let S &lt;- the stack to reverse
Let S' &lt;- an empty stack (the output)
For each item in S
	Pop the head off S into s
	Push s to the head of S'
</code></pre></div></div>

<h2 id="reversing-a-linked-list">Reversing a linked list</h2>

<p>Iterate over the linked list from the head, and for each element in the list to reverse, set the item as the predecessor of the head in the new reversed list</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Let L &lt;- the linked list to reverse
Let L' &lt;- an empty linked list (the output)
For each item in S
	Let l &lt;- the first item in the linked list
	Delete the first item in the linked list
	Add l as the head of L'
</code></pre></div></div>

<h1 id="set-operations">Set operations</h1>

<h2 id="generic-merging">Generic merging</h2>

<p>Taking the union of two sets, in linear time:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Let A, B &lt;- The lists to merge
Let S &lt;- an empty list (the output)
While neither A nor B are empty
	Let a, b &lt;- The first elements of A and B respectively
	If a &lt; b
		Add a to the end of S
		Remove a from A
	Else if b &lt; a
		Add b to the end of S
		Remove b from B
	Else (hence a=b)
		Add a to the end of S (both are equal, so it doesn't matter which)
		Remove a and b from A and B respectively
(Cleaning up the other list when one is empty)
While A is not empty
	Add all the items left in A to the end of S
While B is not empty
	Add all the items left in B to the end of S
</code></pre></div></div>

<h1 id="graph-algorithms">Graph algorithms</h1>

<h2 id="depth-first-search">Depth-first search</h2>

<blockquote>
  <p><strong>Algorithm</strong> \(DFS(G, v)\)
		<strong>Input</strong>  graph \(G\) and start at vertex \(v\) of \(G\)
		<strong>Output</strong> labeling of the edges of \(G\) in the connected component of v as discovery edges and back edges
		\(setLabel(v, VISITED)\)
		<strong>for all</strong> \(e \in G.incidentEdges(v)\)
		    <strong>if</strong> \(getLabel(e) = UNEXPLORED\)
		 		   \(w \leftarrow opposite(v,e)\)
		 		   <strong>if</strong> \(getLabel(w) = UNEXPLORED\)
		 		 		  \(setLabel(e, DISCOVERY)\)
		 		 		  \(DFS(G, w)\)
		 		   <strong>else</strong>
		 		 		  \(setLabel(e,BACK)\)
<strong>END ALGORITHM</strong></p>
</blockquote>

<h3 id="dfs-for-an-entire-graph">DFS for an entire graph:</h3>

<p>The following algorithm is pseudocode for Depth First Search - as displayed by the CS126 lectures, which is used to perform depth first search on the entire graph.</p>

<blockquote>
  <p><strong>Algorithm</strong> \(DFS(G)\)
		<strong>Input</strong> graph \(G\)
		<strong>Output</strong> labelling of the edges of \(G\) as discovery and back edges
		<strong>for all</strong> \(u \in G.vertices()\)
		    <strong>\(setLabel(u, UNEXPLORED)\)</strong>
		<strong>for all</strong> \(e \in G.edges()\)
		    <strong>\(setLabel(e, UNEXPLORED)\)</strong>
		<strong>for all</strong> \(u \in G.vertices()\)
		    <strong>if \(getLabel(u, UNEXPLORED)\)</strong>
		 		   \(DFS(G, v)\)
<strong>END ALGORITHM</strong></p>
</blockquote>

<p>Along with starting at a given vertex:</p>

<blockquote>
  <p><strong>Algorithm</strong> \(DFS(G, v)\)
		<strong>Input</strong>  graph \(G\) and start at vertex \(v\) of \(G\)
		<strong>Output</strong> labeling of the edges of \(G\) in the connected component of v as discovery edges and back edges
		\(setLabel(v, VISITED)\)
		<strong>for all</strong> \(e \in G.incidentEdges(v)\)
		    <strong>if</strong> \(getLabel(e) = UNEXPLORED\)
		 		   \(w \leftarrow opposite(v,e)\)
		 		   <strong>if</strong> \(getLabel(w) = UNEXPLORED\)
		 		 		  \(setLabel(e, DISCOVERY)\)
		 		 		  \(DFS(G, w)\)
		 		   <strong>else</strong>
		 		 		  \(setLabel(e,BACK)\)
<strong>END ALGORITHM</strong></p>
</blockquote>

<h3 id="path-finding-with-dfs">Path Finding with DFS</h3>

<p>By using an alteration of the depth first search algorithm, we can use it to find a path between two given vertices, using the <strong>template method pattern</strong></p>

<blockquote>
  <p><strong>Algorithm</strong>
\(pathDFS(G,v,z)\)
		\(setLabel(v, VISITED)\)
		\(S.push(v)\)
		<strong>if</strong> \(v=z\)
		    <strong>return</strong> \(S.elements()\)
		<strong>for all</strong> \(e \in G.incidentEdges(v)\)
		    <strong>if</strong> \(getLabel(e) = UNEXPLORED\)
		 		   \(w \leftarrow opposite(v,e)\)
		 		   <strong>if</strong> \(getLabel(w) = UNEXPLORED\)
		 		 		  \(setLabel(e,DISCORVERY)\)
		 		 		  \(S.push(e)\)
		 		 		  \(pathDFS(G,w,z)\)
		 		 		  \(S.pop(e)\)
		 		   <strong>else</strong>
		 		 		  \(setLabel(e, BACK)\)
		    \(S.pop(v)\)
<strong>END ALGORITHM</strong></p>
</blockquote>

<h3 id="cycle-finding-with-dfs">Cycle Finding with DFS</h3>

<p>The algorithm for DFS can be adapted slightly in order to find a simply cycle back to the start node.</p>

<blockquote>
  <p><strong>Algorithm</strong> \(cycleDFS(G,v)\)
		\(setLabel(v,VISITED)\)
		\(S.push(v)\)
		<strong>for all</strong> \(e \in G.incidentEdges(v)\)
		    <strong>if</strong> \(getLabel(e) = UNEXPLORED\)
		 		   \(w \leftarrow opposite(v,e)\)
		 		   \(S.push(e)\)
		 		   <strong>if</strong> \(getLabel(w)= UNEXPLORED\)
		 		 		  <strong>if</strong> \(setLabel(e,DISCOVERY)\)
		 		 		  \(cycleDFS(G,w)\)
		 		 		  \(S.pop(e)\)
		 		   <strong>else</strong>
		 		 		  <strong>T</strong> \(\leftarrow\) new empty stack
		 		 		  <strong>repeat</strong>
		 		 		 		 \(o \leftarrow S.pop()\)
		 		 		 		 \(T.push(o)\)
		 		 		  <strong>until</strong> \(o=w\)
		 		 		  <strong>return</strong> \(T.elements()\)
		\(S.pop(v)\)
<strong>END ALGORITHM</strong></p>
</blockquote>

<h3 id="topological-ordering-using-dfs">Topological ordering using DFS</h3>

<blockquote>
  <p><strong>Algorithm</strong> \(topologicalDFS(G)\)
		<strong>Input</strong> dag \(G\)
		<strong>Output</strong> topotlogical ordering of G
		\(n \leftarrow G.numVertices()\)
		<strong>for all</strong> \(u\in G.vertices()\)
		    \(setLabel(,UNEXPLORED)\)
		<strong>for all</strong> \(v\in G.vertices()\)
		    <strong>if</strong> \(getLabel(v) = UNEXPLORED\)
		 		   \(topologicalDFS(G,v)\)
<strong>END ALGORITHM</strong></p>
</blockquote>

<blockquote>
  <p><strong>Algorithm</strong> \(topologicalDFS(G,v)\)
		<strong>Input</strong> graph \(G\) and start a vertex \(v\) of \(G\)
		<strong>Output</strong> labeling of the vertices of G in the connected component of \(v\)
		\(setLabel(v, VISITED)\)
		<strong>for all</strong> \(e\in G.outEdges(v)\)
		    \(w\in opposite(v,e)\) // Outgoing edges
		    <strong>if</strong> \(getLabel(w) = UNEXPLORED\)
		 		   \(topologicalDFS(G,w)\) // \(e\) is a discovery edge
		    <strong>else</strong>
		 		   Label \(v\) with topological number \(n\) // \(e\) is a forward or cross edge
		    \(n\leftarrow n - 1\)
<strong>END ALGORITHM</strong></p>
</blockquote>

<h2 id="breadth-first-search">Breadth-first search</h2>

<blockquote>
  <p><strong>Algorithm</strong> \(BFS(G)\)
		<strong>Input</strong> graph \(G\)
		<strong>Output</strong> labeling of the edges and partition of the vertices of G
		<strong>for all</strong> \(e \in G.vertices()\)
		    \(setLabel(u, UNEXPLORED)\)
		<strong>for all</strong> \(e \in G.edges()\)
		    \(setLabel(e, UNEXPLORED)\)
		<strong>for all</strong> \(v \in G.vertices()\)
		    <strong>if</strong> \(getLabel(v) = UNEXPLORED\)
		 		   \(BFS(G,v)\)
<strong>END ALGORITHM</strong></p>
</blockquote>

<blockquote>
  <p><strong>Algorithm</strong> \(BFS(G, s)\)
		\(L_0 \leftarrow\) new empty sequence
		\(L_0 .addLast(s)\)
		\(setLabel(s, VISITED)\)
		\(i \leftarrow 0\)
		<strong>while</strong> \(¬L_i .isEmpty()\)
		    \(L_i+1 \leftarrow\) new empty sequence
		    <strong>for all</strong> \(v\in L_i .elements()\)
		 		   <strong>for all</strong> \(e \in G.incidentEdges(v)\)
		 		 		  <strong>if</strong> \(getLabel(e) = UNEXPLORED\)
		 		 		 		 \(w \leftarrow opposite(v,e)\)
		 		 		 		 <strong>if</strong> \(getLabel(w) = UNEXPLORED\)
		 		 		 		 		\(setLabel(e) = (e, DISCOVERY)\)
		 		 		 		 		\(setLabel(w,VISITED)\)
		 		 		 		 		\(L_i+1 .addLast(w)\)
		 		 		 		 <strong>else</strong>
		 		 		 		 		\(setLabel(e,CROSS)\)
		    \(i \leftarrow i + 1\)
<strong>END ALGORITHM</strong></p>
</blockquote>

<h2 id="directed-graphs">Directed graphs</h2>

<blockquote>
  <p><strong>Algorithm</strong> \(FloydWarshall(G)\)
		<strong>Input</strong> digraph \(G\)
		<strong>Output</strong> transitive closure \(G^*\) of \(G\)
		\(i \leftarrow 1\)
		<strong>for all</strong> \(v \in G.vertices()\)
		    denote \(v\) as \(v_i\)
		    \(i \leftarrow i + 1\)
		\(G_0 \leftarrow G\)
		<strong>for</strong> \(k \leftarrow 1\) <strong>to</strong> \(n\) <strong>do</strong>
		    \(G_k \leftarrow G_{k-1}\)
		 		   <strong>for</strong> \(i\leftarrow 1\) <strong>to</strong> \(n(i\neq k)\) <strong>do</strong>
		 		 		  <strong>for</strong> \(j \leftarrow 1\) <strong>to</strong> \(n(j\neq i, k)\) <strong>do</strong>
		 		 		 		 <strong>if</strong> \(G_{k-1}.areAdjacent(v_i,v_k)\)  \(\&amp;\) \(G_{k-1}.areAdjacent(v_k,v_j)\)
		 		 		 		 		<strong>if</strong> \(¬G_{k-1}.areAdjacent(v_i,v_j)\)
		 		 		 		 		    \(G_k.insertDirectedEdge(v_i,v_j,k)\)
		    <strong>return</strong> \(G_n\)
**END ALGORITHM</p>
</blockquote>

<h1 id="miscellaneous">Miscellaneous</h1>

<h2 id="computing-spans">Computing spans</h2>

<p>The span of an array is the maximum number of consecutive elements less than a value at an index which precede it
This can be calculated in linear time by</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Let X &lt;- the array to find spans of
Let S &lt;- a stack of all the indices in X
Let i be the current index
Pop indices from the stack until we find index j such that X[i] &lt; X[j]
Set S[i] &lt;- i-j
Push i to the stack
</code></pre></div></div>

<h2 id="fibonacci">Fibonacci</h2>

<h3 id="exponential-time">Exponential time</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Function fibonacci(k)
	If k = 1
		Return k
	Else
		Return fibonacci(k-1) + fibonacci(k-2)
</code></pre></div></div>

<p>This is very inefficient, running in \(O(2^n)\) time, since it re-calculates calls to <code class="language-plaintext highlighter-rouge">fibonacci(k)</code> for some <code class="language-plaintext highlighter-rouge">k</code> many times, instead of using the same result every time it is needed</p>

<h3 id="linear-time">Linear time</h3>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>//Returns the tuple (f_k, f_k-1)
Function fibonacci(k)
	If k = 1
		Return (k, 0)
	Else
		Let i,j &lt;- fibonacci(k - 1)
		Return (i-j, i)
</code></pre></div></div>


                
                <footer class="site-footer">
                    
                    <span class="site-footer-owner"><a href="https://github.com/CSRG-Group/dcs-notes.github.io">dcs-notes.github.io</a> is maintained by <a href="https://github.com/CSRG-Group">CSRG-Group</a>.</span>
                    
                </footer>
            </main>
        </div>
    </div>
</body>

</html>